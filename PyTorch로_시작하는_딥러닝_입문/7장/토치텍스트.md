# 토치텍스트(TorchText)의 batch_first

토치텍스트에서 배치퍼스트(batch_first)를 True로 한 경우와 False를 한 경우를 비교해보겠습니다

### 1.훈련 데이터와 테스트 데이터로 분리하기

우선 IMDB리뷰 데이터를 다운 
```py
import urllib.request
import pandas as pd

urllib.request.urlretrieve("https://raw.githubusercontent.com/LawrenceDuan/IMDb-
Review-Analysis/master/IMDb_Reviews.csv", filename="IMDb_Reviews.csv")
```

다운로드한 IMDB 리뷰 데이터를 데이터프레임에 저장
```py
df = pd.read_csv('IMDb_Reviews.csv', encoding='latin1')
전체 샘플의 개수를 보겠습니다.
print('전체 샘플의 개수 : {}'.format(len(df)))
```
```
전체 샘플의 개수 : 50000
```

전체 샘플의 개수는 50,000개입니다 25,000개씩 
```py
train_df = df[:25000]
test_df = df[25000:]

train_df.to_csv("train_data.csv", index=False)
test_df.to_csv("test_data.csv", index=False)
```

### 2.필드정의하기

torchtext.data를 통해 필드를 정의합니다
```py
from torchtext import data # torchtext.data 임포트

# 필드 정의
TEXT = data.Field(sequential=True,
                  use_vocab=True,
                  tokenize=str.split,
                  lower=True,
                  batch_first=True, # <== 이 부분을 True로 합니다.
                  fix_length=20)

LABEL = data.Field(sequential=False,
                   use_vocab=False,
                   batch_first=False,
                   is_target=True)
```

### 3.데이터셋/단어집합/데이터로더 만들기
```py
from torchtext.data import TabularDataset
from torchtext.data import Iterator

# TabularDataset은 데이터를 불러오면서 필드에서 정의했던 토큰화 방법으로 토큰화를 수행합니다.
train_data, test_data = TabularDataset.splits(
        path='.', train='train_data.csv', test='test_data.csv', format='csv',
        fields=[('text', TEXT), ('label', LABEL)], skip_header=True)

# 정의한 필드에 .build_vocab() 도구를 사용하면 단어 집합을 생성합니다.
TEXT.build_vocab(train_data, min_freq=10, max_size=10000) # 10,000개의 단어를 가진 단어 집합 생성

# 배치 크기를 정하고 첫번째 배치를 출력해보겠습니다.
batch_size = 5
train_loader = Iterator(dataset=train_data, batch_size = batch_size)
batch = next(iter(train_loader)) # 첫번째 미니배치

print(batch.text)
```
```
tensor([[  31,  191,   24,  133,  445,  115,   42,   10,  149,    2, 3581, 6601,
            0,   12,  172,   74,  358,  806,    6,  425],
        [   9,   98,   12,   10,   20,    7,  157, 2520,  285,   11, 1384,   46,
          921, 4255,   16,   10,    0,  702,   82,    5],
        [   9,  323,  148,   10,   25,   17,  110, 3109,   80,   44,  291, 4427,
            3,  778, 3286,   17,    0,    2, 1308,  193],
        [  10,    7,   49, 8950,   18,  189,  184,    5,    2, 1890,   17,   10,
            0,  118,   24,   62,  141,    2,  162,   16],
        [   9,  574, 4312, 1147,   64, 2621,    3,  283,  499,   16,   21,  138,
            0,    5,    0, 5994,    2, 1462,   12,    2]])
```
```
print(batch.text.shape)
```
```
torch.Size([5, 20])
```

### 4.필드재정의하기
```py
# 필드 정의
TEXT = data.Field(sequential=True,
                  use_vocab=True,
                  tokenize=str.split,
                  lower=True,
                  fix_length=20)

LABEL = data.Field(sequential=False,
                   use_vocab=False,
                   batch_first=False,
                   is_target=True)
```

### 5. batch_first = False로 하였을 경우의 텐서 크기
```py
# 첫번째 미니 배치 출력
print(batch.text)
```
```
tensor([[  31,    9,    9,   10,    9],
        [ 191,   98,  323,    7,  574],
        [  24,   12,  148,   49, 4312],
        [ 133,   10,   10, 8950, 1147],
        [ 445,   20,   25,   18,   64],
        [ 115,    7,   17,  189, 2621],
        [  42,  157,  110,  184,    3],
        [  10, 2520, 3109,    5,  283],
        [ 149,  285,   80,    2,  499],
        [   2,   11,   44, 1890,   16],
        [3581, 1384,  291,   17,   21],
        [6601,   46, 4427,   10,  138],
        [   0,  921,    3,    0,    0],
        [  12, 4255,  778,  118,    5],
        [ 172,   16, 3286,   24,    0],
        [  74,   10,   17,   62, 5994],
        [ 358,    0,    0,  141,    2],
        [ 806,  702,    2,    2, 1462],
        [   6,   82, 1308,  162,   12],
        [ 425,    5,  193,   16,    2]])
```
```py
print(batch.text.shape)
```
```
torch.Size([20, 5])
```
