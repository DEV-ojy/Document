# 사전 훈련된 워드 임베딩(Pretrained Word Embedding)

임베딩 벡터를 얻기 위해서 파이토치의 nn.Embedding()을 사용하기도 하지만 때로 는 이미 훈련되어져 있는 워드 임베딩을 불러서
이를 임베딩 벡터로 사용하기도 합니다 훈련 데이터가 부족한 상황이라면 모델에 파이토치의 nn.Embedding()을 사용하는 것보다 
다른 텍스트 데이터로 사전 훈련되어 있는 임베딩 벡터를 불러오는 것이 나은 선택일 수 있습니다 

## 1. IMDB 리뷰 데이터를 훈련 데이터로 사용하기

토치텍스트에서 제공하는 IMDB리뷰 데이터를 다운받아 이를 사용하겠습니다 
Field 객체를 정의해 줍니다 
```py
from torchtext import data, datasets

TEXT = data.Field(sequential=True, batch_first=True, lower=True)
LABEL = data.Field(sequential=False, batch_first=True)
```

필드를 정의했다면 실습을 위해 필요한 데이터를 준비해야합니다 
torchtext.datasets은 IMDB,TREC(질문분류),언어 모델리(WikiText-2)등 다른 여러 데이터셋을 제공합니다 
torchtext.datasets을 사용해 IMDB 데이터셋을 다운로드하고 이 데이터셋을 학습 데이터셋과 테스트 테이터셋으로 나누겠습닌다

```py
trainset, testset = datasets.IMDB.splits(TEXT, LABEL)
```
```py
print(vars(trainset[0]))
```
```
{'text': ['streisand', 'fans', 'only', 'familiar', 'with', 'her', 'work', 'from', 'the', 'funny', 'girl', 'film', 'onwards', 
        ... 중략 ...
        'ever,', 'done', 'anything', 'better!"<br', '/><br', '/>and', 'she', 'was', 'twenty-three', 'years', 'old!'],
'label': 'pos'}
```

## 2. 토치텍스트를 사용한 사전 훈련된 워드 임베딩
이번에는 토치텍스트를 사용해서 외부에서 가져온 사전 훈련된 워드 임베딩을 사용해봅시다 

### 1. 사전 훈련된 Word2Vec 모델 확인하기
여기서는 앞선 챕터에서 만들어 두었던 모델을 사용하겠습니다 
우선 'eng_w2v' 모델을 로드하여 저자가 임의로 선택한 영어 단어 'this'와 'self-indulgent'의 임베딩 벡터값을 확인해보겠습니다

```py
from gensim.models import KeyedVectors

word2vec_model = KeyedVectors.load_word2vec_format('eng_w2v')

print(word2vec_model['this']) # 영어 단어 'this'의 임베딩 벡터값 출력
```
```
[-0.48601353  2.4053142  -0.8451491  -0.636236   -0.09839931  0.9016884
  ... 중략 ...
  0.06850401  2.321853   -1.2139624  -1.2775816 ]
```
영어 단어 'this'의 임베딩 벡터값이 정상적으로 출력됩니다
```py
print(word2vec_model['self-indulgent']) # 영어 단어 'self-indulgent'의 임베딩 벡터값 출력
```
```
KeyError: "word 'self-indulgent' not in vocabulary"
```

### 2. 사전 훈련된 Word2Vec을 초기 임베딩으로 사용하기
이제 이 임베딩 벡터들을 IMDB리뷰 데이터의 단어들에 맵핑해보겠습니다

```py
import torch
import torch.nn as nn
from torchtext.vocab import Vectors

vectors = Vectors(name="eng_w2v") # 사전 훈련된 Word2Vec 모델을 vectors에 저장

```
Field 객체의 build_vocab을 통해 훈련 데이터의 단어 집합(vocabulary)를 만드는 것과 동시에 임베딩 벡터값들을 초기화할수있습니다

```py
TEXT.build_vocab(trainset, vectors=vectors, max_size=10000, min_freq=10) # Word2Vec 모델을 임베딩 벡터값으로 초기화
```

