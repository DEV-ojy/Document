# 다중 선형 회귀 (Multivariable Linear regression)

앞서 배운 x가 1개인 선형회귀를 단순 선형 회귀(Simple Linear Regression)이라고 합니다
이번 챕터에서는 다수의 x로부터 y를 예측하는 다중 선형 회귀(Multivariable Linear regression)에 대해서 이해해봅시다

## 데이터에 대한 이해(Data Definition)

다음과 같은 훈련 데이터가 있습니다 앞서 배운 단순 선형 회귀와 다른 점은 독립변수 x의 개수가 이제 1개가 아닌 3개라는점입니다
3개의 퀴즈 점수로부터 최종 점수를 예측하는 모델을 만들어보겠습니다

![image](https://user-images.githubusercontent.com/80239748/127311461-8e0c44a9-8fc8-47a5-b65b-56bc0c30f183.png)

독립 변수 x의 개수가 3개미으모 이를 수식으로 표현하면 아래와 같습니다

![image](https://user-images.githubusercontent.com/80239748/127311521-46f3dad2-531a-43cf-b5fa-313c9f21a683.png)

## 파이토치로 구현하기 

우선 필요한 도구들을 임포트하고 랜덤시드를 고정

```py

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

```

이제 훈련데이터를 선언해보겠습니다 

![image](https://user-images.githubusercontent.com/80239748/127311741-29e8608e-7882-4b31-a3f3-a55431f530c4.png)

위의 식을 보면 이번에는 단순 선형 회귀와 다르게 x의 개수가 3개입니다 그러니까 x를 3개 선언합니다

```py

# 훈련 데이터
x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])
x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])
x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])

```

이제 가중치 w와 편향 b를 선언합니다 이때 가중치 w도 3개를 선언해주어야 합니다

```py

# 가중치 w와 편향 b 초기화
w1 = torch.zeros(1, requires_grad=True)
w2 = torch.zeros(1, requires_grad=True)
w3 = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)

```

이제 가설,비용 함수, 옵티마이저를 선언한 후에 경사 하강법을 1,000회 반복합니다

```py

# optimizer 설정
optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):

    # H(x) 계산
    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b

    # cost 계산
    cost = torch.mean((hypothesis - y_train) ** 2)

    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100번마다 로그 출력
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()
        ))

```

위의 경우 가설을 선언하는 부분인 hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b 에서도 
 x_train의 개수만큼 w와 곱해주도록 작성해준 것을 확인할 수 있습니다.
 
## 3. 벡터와 행렬 연산으로 바꾸기 
 
위의 코드를 개선할 수 있는 부분이 있습니다 이번에는 x의 개수가 3개였으니 x1_train, x2_train, x3_train와 w1, w2, w3를 일일히 선언해주었습니다 그런데 x의 개수가 수없이 많을때도 그에 맞는 w값을 선어해야합니다 그런 방식은 굉장히 비효율적이기 때문에 이를 해결하기 위해 행렬 곱셈 연산(or 벡터의 내적)을 사용합니다 

![image](https://user-images.githubusercontent.com/80239748/127480297-242d9d84-169c-4f05-b287-0f1b7f7f4391.png)

위의 그림은 행렬 곱셈 연산 과정에서 벡터의 내적으로 1 x 7 + 2 x 9 + 3 x 11 =58이 되는 과정을 보여줍니다

이행렬 연산이 어떻게 현재 배우고 있는 가설과 상관이 있다는 걸까요?
바로 가설을 벡터와 행렬 연산으로 표현할 수 있기 때문입니다 

### 1.벡터 연산으로 이해하기

![image](https://user-images.githubusercontent.com/80239748/127481563-042f48bd-e9af-4402-94a2-8fc36fb1f87d.png)

위 식은 아래와 같이 두 벡터의 내적으로 표현할 수 있습니다 

![image](https://user-images.githubusercontent.com/80239748/127481633-64eabc35-58a8-4d3b-b7bb-3304b3870ceb.png)

두 벡터를 각각 X와 W로 표현한다면, 가설은 다음과 같습니다 

![image](https://user-images.githubusercontent.com/80239748/127481941-c55f6db1-96a8-493a-9e0f-8cd9995be9ef.png)

x의 개수가 3개였음에도 이제는 X와 W라는 두 개의 변수로 표현된 것을 볼 수 있습니다 

### 2.행렬 연산으로 이해하기

훈련 데이터를 살펴보고, 벡터와 행렬 연산을 통해 가설 H(X)를 표현해보겠습니다 

![image](https://user-images.githubusercontent.com/80239748/127482105-2affff63-c022-4085-91ce-4b7fd045cd06.png)
 
전체 훈련 데이터의 개수를 셀 수 있는 1개의 단위를 샘플이라고 합니다 현재 샘플의 수는 총 5개입니다
각 샘플에서 y를 결정하게 하는 각각의 독립 변수 x를 특성이라고 합니다 현재 특성은 3개입니다 
 
이는 종속 변수 x들의 수가 (샘플의 수 x 특성의 수) = 15개 임을 의미합니다 종속 변수 x들은 (샘플의 수 x 특성의 수)의
크기를 가지는  하나의 행렬로 표현해봅시다 그리고 이 행렬을 X라고 하겠습니다 
 
![image](https://user-images.githubusercontent.com/80239748/127484876-ee7370b0-b90f-416f-841f-d0756ac5fbcf.png)
 
그리고 여기에 가중치 w1,w2,w3을 원소로 하는 벡터를 W라 하고 이를 곱해보겠습니다 
 
![image](https://user-images.githubusercontent.com/80239748/127484961-e7572561-cf78-4dff-a598-60f653676396.png)
 
위의 식은 결과적으로 다음과 같습니다 
 
![image](https://user-images.githubusercontent.com/80239748/127485040-bd876419-ace1-4557-9051-60287ce3e0d5.png)
 
이 가설에 각 샘플에 더해지는 편향 b를 추가해봅시다 샘플 수만큼의 차원을 가지는 편향 벡터 B를 만들어 더합니다 
 
![image](https://user-images.githubusercontent.com/80239748/127485119-c8319253-fa7a-4780-867a-d4cb63d6f29b.png)

위의 식은 결과적으로 다음과 같습니다 

![image](https://user-images.githubusercontent.com/80239748/127485228-c0b4ff99-7e14-47ce-849f-56029785524f.png)ㅍ

결과적으로 전체 훈련 데이터의 가설 연산을 3개으 변수만으로 표현하였습니다 이와 같이 벡터와 행렬 연산은 식을 간단하게 해줄 뿐만 아니라 다수의 샘플의 병렬 연산이므로 속도의 이점을 가집니다 

이를 참고로 파이토치로 구현해봅시다 

## 4. 행렬 연산을 고려하여 파이토치로 구현하기 

이번에는 행렬 연산을 고려하여 파아토치로 재구현해보겠습니다 이번에는 훈련 데이터 또한 행렬로 선언해야 합니다

```py

x_train  =  torch.FloatTensor([[73,  80,  75], 
                               [93,  88,  93], 
                               [89,  91,  80], 
                               [96,  98,  100],   
                               [73,  66,  70]])  
y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])

```
 
이전에 x_train을 3개나 구현했던 것과 다르게 이번에는 x_train 하나에 모든 샘플을 전부 선언하였습니다 
다시말해, (5X3)행렬 X을 선언한 것입니다 

x_train과 y_train의 크기(shape)를 출력해보겠습니다
```py
print(x_train.shape)
print(y_train.shape)
```

```
torch.Size([5, 3])
torch.Size([5, 1])
```

각각 (5 × 3) 행렬과 (5 × 1) 행렬(또는 벡터)의 크기를 가집니다.
이제 가중치 W와 편향 b를 선언합니다.

```py

# 가중치와 편향 선언
W = torch.zeros((3, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

```

여기서 주목하 점은 가중치 W의 크기가 (3x1)벡터라는 점입니다 행렬의 곱셈이 성립되려면 곱셈의 촤측에 있는 행렬의 열의
크기와 우측에 있는 행렬의 행의 크기가 일치해야 합니다 현재 X_train의 행렬의 크기는 (5x3)이며, W 벡터의 크기는 (3x1)이므로 두 행렬과 벡터는 행렬곱이 가능합니다 행렬곱으로 가설을 선언하면 아래와 같습니다 

```py
hypothesis = x_train.matmul(W) + b
```

가설을 행렬곱으로 간단히 정의하였습니다 이는 앞서 x_train과 w의 곱셈이 이루어지는 각 항을 전부 기재하여 가설을 선언했던 것과 대비됩니다 이 경우 사용자가 독립변수 x의 수를 후에 추가적으로 늘리거나 줄이더라도 위의 가설선언 코드를 수정할 필요가
없습니다 이제 해야할 일은 비용 함수와 옵티마이저를 정의하고 정해진 에포크만큼 훈련을 진행하는 일입니다 이를 반영한 전체
코드는 다음과 같습니다 

```py
x_train  =  torch.FloatTensor([[73,  80,  75], 
                               [93,  88,  93], 
                               [89,  91,  80], 
                               [96,  98,  100],   
                               [73,  66,  70]])  
y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])

# 모델 초기화
W = torch.zeros((3, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
# optimizer 설정
optimizer = optim.SGD([W, b], lr=1e-5)

nb_epochs = 20
for epoch in range(nb_epochs + 1):

    # H(x) 계산
    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.
    hypothesis = x_train.matmul(W) + b

    # cost 계산
    cost = torch.mean((hypothesis - y_train) ** 2)

    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(
        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()
    ))
```

##### 21.07.30
