# TF-IDF(Term Frequency-Inverse Document Frequency)

이번에는 DTM내에있는 각 단어에 대한 중요도를 계산할 수 있는 TF-IDF 가중치에 대해서 알아보도록 하겠습니다

F-IDF를 사용하면, 기존의 DTM을 사용하는 것보다 보다 더 많은 정보를 고려하여 문서들을 비교할 수 있습니다 

* 주의할 점은 TF-IDF가 DTM보다 항상 성능이 뛰어나진 않습니다


## 1. TF-IDF(단어 빈도-역 문서 빈도, Term Frequency-Inverse Document Frequency)

TF-IDF(Term Frequency-Inverse Document Frequency)는 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법입니다
사용 방법은 우선 DTM을 만든 후, TF-IDF 가중치를 부여합니다

TF-IDF는 주로 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 쓰일 수 있습니다

TF-IDF는 TF와 IDF를 곱한 값을 의미하는데 이를 식으로 표현해보겠습니다  문서를 d, 단어를 t, 문서의 총 개수를 n이라고 표현할 때 TF, DF, IDF는 각각 다음과 같이 정의할 수 있습니다

### (1) tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수

생소한 글자때문에 어려워보일 수 있지만 잘 생각해보면 TF는 이미 앞에서 구한 적이 있습니다
TF는 앞에서 배운 DTM의 예제에서 각 단어들이 가진 값들입니다 DTM이 각 문서에서의 각 단어의 등장 빈도를 나타내는 값이었기 때문입니다

### (2) df(t) : 특정 단어 t가 등장한 문서의 수

여기서 특정 단어가 각 문서, 또는 문서들에서 몇 번 등장했는지는 관심가지지 않으며 오직 특정 단어 t가 등장한 문서의 수에만 관심을 가집니다 앞서 배운 DTM에서 바나나는 문서2와 문서3에서 등장했습니다 이 경우, 바나나의 df는 2입니다. 문서3에서 바나나가 두 번 등장했지만, 그것은 중요한 게 아닙니다 심지어 바나나란 단어가 문서2에서 100번 등장했고, 문서3에서 200번 등장했다고 하더라도 바나나의 df는 2가 됩니다

### (3) idf(d, t) : df(t)에 반비례하는 수

![image](https://user-images.githubusercontent.com/80239748/139573796-18cc3f44-7ad8-4429-9762-7f4d3592b86c.png)

IDF라는 이름을 보고 DF의 역수가 아닐까 생각했다면, IDF는 DF의 역수를 취하고 싶은 것이 맞습니다 

그런데 log와 분모에 1을 더해주는 식에 의아하실 수 있습니다  log를 사용하지 않았을 때, IDF를 DF의 역수로 사용한다면 총 문서의 수 n이 커질 수록, IDF의 값은 기하급수적으로 커지게 됩니다
그렇기 때문에 log를 사용합니다 

왜 log가 필요한지 n=1,000,000일 때의 예를 들어봅시다 log의 밑은 10을 사용한다고 가정하였을 때 결과는 아래와 같습니다

![image](https://user-images.githubusercontent.com/80239748/139573875-636b47bd-f69a-465f-a2bf-c025835c27e4.png)

그렇다면 log를 사용하지 않으면 idf의 값이 어떻게 커지는지 보겠습니다 

![image](https://user-images.githubusercontent.com/80239748/139573882-78b1a5a0-23f2-4102-adb9-003324016f47.png)

또 다른 직관적인 설명은 불용어 등과 같이 자주 쓰이는 단어들은 비교적 자주 쓰이지 않는 단어들보다 최소 수십 배 자주 등장합니다 그런데 비교적 자주 쓰이지 않는 단어들조차 회귀 단어들과 비교하면 또 최소 수백배는 더 자주 등장하는 편입니다 

이때문에 log를 씌워주지 않으면, 희귀단어들에 엄청난 가중치가 부여될 수 있습니다 **로그를 씌우면 이런 격차를 줄이는 효과가 있습니다**

-----------------

또한 log 안의 식에서 분모에 1을 더해주는 이유는 첫번째 이유로는 특정 단어가 전체 문서에서 등장하지 않을 경우에 분모가 0이 되는 상황을 방지하기 위함입니다

TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단합니다 TF-IDF 값이 낮으면 중요도가 낮은 것이며, TF-IDF 값이 크면 중요도가 큰 것입니다 

* 즉, the나 a와 같이 불용어의 경우에는 모든 문서에 자주 등장하기 마련이기 때문에 자연스럽게 불용어의 TF-IDF의 값은 다른 단어의 TF-IDF에 비해서 낮아지게 됩니다

![image](https://user-images.githubusercontent.com/80239748/139573980-9e88f688-db23-4a05-9aa0-755099f018b1.png)

앞서 DTM을 설명하기위해 들었던 위의 예제를 가지고 TF-IDF에 대해 이해해보도록 하겠습니다 
우선 TF는 앞서 사용한 DTM을 그대로 사용하면, 그것이 각 문서에서의 각 단어의 TF가 됩니다

그렇다면 이제 구해야할 것은 TF와 곱해야할 값인 IDF입니다 로그는 자연 로그를 사용하도록 하겠습니다

자연 로그는 로그의 밑을 자연 상수 e(e=2.718281...)를 사용하는 로그를 말합니다  IDF 계산을 위해 사용하는 로그의 밑은 TF-IDF를 사용하는 사용자가 임의로 정할 수 있는데, 여기서 로그는 마치 기존의 값에 곱하여 값의 크기를 조절하는 상수의 역할을 합니다 

그런데 보통 각종 프로그래밍 언어나 프로그램에서 패키지로 지원하는 TF-IDF의 로그는 대부분 자연 로그를 사용합니다 그렇기 때문에 저 또한 자연 로그를 사용하도록 하겠습니다 자연 로그는 보통 log라고 표현하지 않고, ln이라고 표현합니다

![image](https://user-images.githubusercontent.com/80239748/139574143-38f0e247-73d3-4b27-984e-e6ab0a31d202.png)

문서의 총 수는 4이기 때문에 ln 안에서 분자는 늘 4으로 동일합니다 분모의 경우에는 각 단어가 등장한 문서의 수(DF)를 의미하는데, 예를 들어서 '먹고'의 경우에는 총 2개의 문서(문서1, 문서2)에 등장했기 때문에 2라는 값을 가집니다 

각 단어에 대해서 IDF의 값을 비교해보면 문서 1개에만 등장한 단어와 문서2개에만 등장한 단어는 값의 차이를 보입니다 IDF는 여러 문서에서 등장한 단어의 가중치를 낮추는 역할을 하기 때문입니다

그러면 이제 TF-IDF를 계산해보도록 하겠습니다 TF는 DTM을 그대로 가져오면 각 문서에서의 각 단어의 TF를 가져오게 되기 때문에, 앞서 사용한 DTM에서 단어 별로 위의 IDF값을 그대로 곱해주면 TF-IDF가 나오게 됩니다

![image](https://user-images.githubusercontent.com/80239748/139663800-d8954d05-fa94-4af7-8561-a2389157051f.png)

사실 예제 문서가 굉장히 간단하기 때문에 계산은 매우 쉽습니다 문서3에서의 바나나만 TF 값이 2이므로 IDF에 2를 곱해주고, 나머진 TF 값이 1이므로 그대로 IDF 값을 가져오면 됩니다 문서2에서의 바나나의 TF-IDF 가중치와 문서3에서의 바나나의 TF-IDF 가중치가 다른 것을 볼 수 있습니다

 수식적으로 말하면, TF가 각각 1과 2로 달랐기 때문인데 TF-IDF에서의 관점에서 보자면 TF-IDF는 특정 문서에서 자주 등장하는 단어는 그 문서 내에서 중요한 단어로 판단하기 때문입니다 문서2에서는 바나나를 한 번 언급했지만, 문서3에서는 바나나를 두 번 언급했기 때문에 문서3에서의 바나나를 더욱 중요한 단어라고 판단하는 것입니다

 ## 2. 파이썬으로 TF-IDF 직접 구현하기 

필요한 도구를 먼저 임포트합니다 

```py
import pandas as pd # 데이터프레임 사용을 위해
from math import log # IDF 계산을 위해
```
앞의 설명에서 사용한 4개의 문서를 docs에 저장합니다 
```py
docs = [
  '먹고 싶은 사과',
  '먹고 싶은 바나나',
  '길고 노란 바나나 바나나',
  '저는 과일이 좋아요'
] 
vocab = list(set(w for doc in docs for w in doc.split()))
vocab.sort()
```
TF,IDF그리고 TF-IDF값을 구하는 함수를 구현합니다 
```py
N = len(docs) # 총 문서의 수

def tf(t, d):
    return d.count(t)

def idf(t):
    df = 0
    for doc in docs:
        df += t in doc
    return log(N/(df + 1))

def tfidf(t, d):
    return tf(t,d)* idf(t)
```
이제 TF를 구해보겠습니다 다시 말해 DTM을 데이터프레임에 저장하여 출력해보겠습니다 
```py
result = []
for i in range(N): # 각 문서에 대해서 아래 명령을 수행
    result.append([])
    d = docs[i]
    for j in range(len(vocab)):
        t = vocab[j]        
        result[-1].append(tf(t, d))

tf_ = pd.DataFrame(result, columns = vocab)
tf_
```
정상적으로 DTM이 출력되었습니다 이제 각 단어에 대한 IDF값을 구해봅시다 
```py
result = []
for j in range(len(vocab)):
    t = vocab[j]
    result.append(idf(t))

idf_ = pd.DataFrame(result, index = vocab, columns = ["IDF"])
idf_
```
위에서 수기로 구한 IDF값들과 정확히 일치합니다 이제 TF-IDF행렬을 출력해봅시다 
```py
result = []
for i in range(N):
    result.append([])
    d = docs[i]
    for j in range(len(vocab)):
        t = vocab[j]

        result[-1].append(tfidf(t,d))

tfidf_ = pd.DataFrame(result, columns = vocab)
tfidf_
```
지금까지 TF-IDF의 가장 기본적인 식에 대해서 학습하고, 이를 실제로 구현하는 실습을 진행해보았습니다 그런데 사실 실제 TF-IDF 구현을 제공하고 있는 많은 패키지들은 패키지마다 식이 조금씩 다르긴 하지만, 위에서 배운 기본 식에서 조정된 식을 사용합니다

그 이유는 위의 기본적인 식을 바탕으로 한 구현에도 여전히 문제점이 존재하기 때문입니다
만약 전체 문서의 수 n이 4인데, df(t)의 값이 3인 경우에는 어떤 일이 벌어질까요? df(t)에 1이 더해지면서 log항의 분자와 분모의 값이 같아지게 됩니 

이는 log의 진수값이 1이 되면서 idf(d,t)의 값이 0이 됨을 의미합니다 식으로 표현하면 idf(d,t) = log(n/(df(t)+1))=0입니다 IDF의 값이 0이라면 더 이상 가중치의 역할을 수행하지 못합니다 

그래서 실제 구현체는 idf(d,t) = log(n/(df(t)+1))+1과 같이 log항에 1을 더해줘서 log항의 값이 0이되더라도 IDF가 치소 1이상의 값을 가지도록 합니다 
```py

```
```py

```


