# NLP에서의 사전 훈련(Pre-training)

2018년 딥 러닝 연구원 세바스찬 루더는 사전 훈련된 언어 모델의 약진을 보며 다음과 같은 말을 했습니다

    "사전 훈련된 단어 임베딩이 모든 NLP 실무자의 도구 상자에서 사전 훈련된 언어 모델로 대체되는 것은 시간 문제이다."

BERT(Bidirectional Encoder Representations from Transformers)와 같은 트랜스포머 계열의 모델들이 자연어 처리를 지배했던 19년과 20년을 회고하면 이 말은 이미 현실이 되었습니다
BERT를 배우기에 앞서 워드 임베딩에서부터 ELMo, 그리고 트랜스포머에 이르기까지 자연어 처리가 발전되어온 흐름을 정리해봅시다!

## 1. 사전 훈련된 워드 임베딩

앞서 Word2Vec, FastText, GloVe와 같은 워드 임베딩 방법론들을 학습했습니다
어떤 태스크를 수행할 때, 임베딩을 사용하는 방법으로는 크게 두 가지가 있습니다 
`임베딩 층(Embedding layer)을 랜덤 초기화하여 처음부터 학습하는 방법` 과 `방대한 데이터로 Word2Vec 등과 같은 임베딩 알고리즘으로 사전에 학습된 임베딩 벡터들을 가져와 사용하는 방법`이 있습니다 

만약 , 태스크에 사용하기 위한 데이터가 적다면, 사전 훈련된 임베딩을 사용하면 성능 향상을 기대해볼 수 있었습니다

그런데 이 두 가지 방법 모두 해결하지 못한 문제점이 존재했는데 하나의 단어가 하나의 벡터값으로 맵핑되므로 문맥을 고려하지 못한다는 점이었습니다 단적으로, 다의어나 동음이의어를 구분하지 못하는 문제점이 있었습니다

한국어에는 '사과'라는 단어가 존재하는데 이 '사과'는 용서를 빈다는 의미로도 쓰이지만, 먹는 과일의 의미로도 사용됩니다

그러나 임베딩 벡터는 '사과'라는 벡터에 하나의 벡터값을 맵핑하므로 이 두 가지 의미를 구분할 수 없었습니다

이 한계는 사전 훈련된 언어 모델을 사용하므로서 극복할 수 있었는데, 이는 아래 ELMo에서 다시 재언급하겠습니다

## 2. 사전 훈련된 언어 모델

![image](https://user-images.githubusercontent.com/80239748/146754465-78718ae1-6d0c-4581-84c7-eb8312f22ebf.png)

2015년 구글은 `Semi-supervised Sequence Learning`라는 논문에서 LSTM 언어 모델을 학습하고나서 이렇게 학습한 LSTM을 텍스트 분류에 추가 학습하는 방법을 보였습니다
이 방법은 우선 LSTM 언어 모델을 학습합니다 언어 모델은 주어진 텍스트로부터, 이전 단어들로
부터 다음 단어를 예측하도록 학습하므로 기본적으로 `별도의 레이블이 없는 데이터로 학습된 LSTM`과 `가중치가 랜덤으로 초기화 된 LSTM`을 두고 텍스트 분류와 같은 문제를 학습하여 사전 훈련된 언어 모델을 사용한 전자의 경우가 더 좋은 성능을 얻을 수 있다는 가능성을 보였습니다

방대한 텍스트로 LSTM 언어 모델을 학습해두고, 언어 모델을 다른 태스크에 사용하는 방법으로는 앞서 이전에 학습한 ELMo와 같은 아이디어도 존재합니다

![image](https://user-images.githubusercontent.com/80239748/146755046-e6425bf4-fd57-4b55-b43c-5811b0c4a83d.png)

ELMo는 순방향 언어 모델과 역방향 언어 모델을 각각 따로 학습시킨 후에, 이렇게 사전 학습된 언어 모델로부터 임베딩 값을 얻는다는 아이디어였습니다 
이러한 임베딩은 문맥에 따라서 임베딩 벡터값이 달라지므로, 기존 워드 임베딩인 **Word2Vec이나 GloVe 등이 다의어를 구분할 수 없었던 문제점을 해결할 수 있었습니다**

LSTM을 대체할 모델로 트랜스포머가 등장하자 언어 모델을 학습할 때 LSTM이 아닌 트랜스포머를 사용하는 시도가 등장하기 시작했습니다

![image](https://user-images.githubusercontent.com/80239748/146756100-3fc9423f-7aad-4fe7-af8b-e683e82de189.png)

위의 그림에서 Trm은 트랜스포머(Transformer)의 약자입니다 트랜스포커의 디코더는  LSTM 언어 모델처럼 순차적으로 이전 단어들로부터 다음 단어를 예측합니다

Open AI는 트랜스포머 디코더로 총 12개의 층을 쌓은 후에 방대한 텍스트 데이터를 학습시킨 언어 모델 GPT-1을 만들었습니다 Open AI는 GPT-1에 여러 다양한 태스크를 위해 추가 학습을 진행하였을 때, 각 태스크에서 기존보다 높은 성능을 얻을 수 있음을 입증했습니다

본격적으로 NLP의 주요 트렌드는 사전 훈련된 언어 모델을 만들어 이를 특정 태스크에 추가 학습시키므로서 해당 태스크에서 높은 성능을 얻는 것이 되었고, 언어 모델의 구조에도 변화가 오기 시작했습니다

![image](https://user-images.githubusercontent.com/80239748/146921584-903e745c-13a3-4bad-aed9-c5b1c3eb5d12.png)

위의 좌측 그림에 있는 단방향 언어모델은 지금까지 배운 전형적인 언어 모델입니다
시작 토큰 <SOS>가 들어가면, 다음 단어 I를 예측하고 그리고 그 다음 단어 am을 예측합니다

반면, 우측에 있는 양방향 언어 모델은 본 적 없던 형태의 언어 모델입니다
실제로 이렇게 구현하는 경우는 거의 없는데 그 이유가 무엇일까요? 가령, 양방향 LSTM을 이용해서 우측과 같은 언어 모델을 만들었다고 해봅시다

초록색 LSTM 셀은 순방향 언어 모델로 <sos>를 입력받아 I를 예측하고, 그 후에 am을 예측합니다 그런데 am을 예측할 때, 출력층은 주황색 LSTM 셀인 역방향 언어 모델의 정보도 함께 받고있습니다

그런데 am을 예측하는 시점에서 역방향 언어 모델이 이미 관측한 단어는 a, am, I 이렇게 3개의 단어입니다
이미 예측해야하는 단어를 역방향 언어 모델을 통해 관측한 셈이므로 언어 모델은 일반적으로 양방향으로 구현하지 않습니다

하지만 언어의 문맥이라는 것은 실제로는 양방향입니다

텍스트 분류나 개체명 인식 등에서 양방향 LSTM을 사용하여 모델을 구현해서 좋은 성능을 얻을 수 있었던 것을 상기해봅시다

하지만 이전 단어들로부터 다음 단어를 예측하는 언어 모델의 특성으로 인해 양방향을 사용할 수 없으므로, 그 대안으로 ELMo에서는 정방향과 역방향이라는 방향이 다른 두 개의 단방향 언어 모델을 준비하여 학습하는 방법을 사용했던 것입니다

    이와 같이 기존 언어 모델로는 양방향 구조를 도입할 수 없으므로, 양방향 구조를 도입하기 위해서 2018년에는 새로운 구조의 언어 모델이 탄생했는데 바로 마스크드 언어 모델입니다









