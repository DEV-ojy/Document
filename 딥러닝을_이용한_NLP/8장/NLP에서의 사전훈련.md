# NLP에서의 사전 훈련(Pre-training)

2018년 딥 러닝 연구원 세바스찬 루더는 사전 훈련된 언어 모델의 약진을 보며 다음과 같은 말을 했습니다

    "사전 훈련된 단어 임베딩이 모든 NLP 실무자의 도구 상자에서 사전 훈련된 언어 모델로 대체되는 것은 시간 문제이다."

BERT(Bidirectional Encoder Representations from Transformers)와 같은 트랜스포머 계열의 모델들이 자연어 처리를 지배했던 19년과 20년을 회고하면 이 말은 이미 현실이 되었습니다
BERT를 배우기에 앞서 워드 임베딩에서부터 ELMo, 그리고 트랜스포머에 이르기까지 자연어 처리가 발전되어온 흐름을 정리해봅시다!

## 1. 사전 훈련된 워드 임베딩

앞서 Word2Vec, FastText, GloVe와 같은 워드 임베딩 방법론들을 학습했습니다
어떤 태스크를 수행할 때, 임베딩을 사용하는 방법으로는 크게 두 가지가 있습니다 
`임베딩 층(Embedding layer)을 랜덤 초기화하여 처음부터 학습하는 방법` 과 `방대한 데이터로 Word2Vec 등과 같은 임베딩 알고리즘으로 사전에 학습된 임베딩 벡터들을 가져와 사용하는 방법`이 있습니다 

만약 , 태스크에 사용하기 위한 데이터가 적다면, 사전 훈련된 임베딩을 사용하면 성능 향상을 기대해볼 수 있었습니다

그런데 이 두 가지 방법 모두 해결하지 못한 문제점이 존재했는데 하나의 단어가 하나의 벡터값으로 맵핑되므로 문맥을 고려하지 못한다는 점이었습니다 단적으로, 다의어나 동음이의어를 구분하지 못하는 문제점이 있었습니다

한국어에는 '사과'라는 단어가 존재하는데 이 '사과'는 용서를 빈다는 의미로도 쓰이지만, 먹는 과일의 의미로도 사용됩니다

그러나 임베딩 벡터는 '사과'라는 벡터에 하나의 벡터값을 맵핑하므로 이 두 가지 의미를 구분할 수 없었습니다

이 한계는 사전 훈련된 언어 모델을 사용하므로서 극복할 수 있었는데, 이는 아래 ELMo에서 다시 재언급하겠습니다