# 센텐스버트(Sentence BERT, SBERT)

BERT로부터 문장 임베딩을 얻을 수 있는 센텐스버트(Sentence BERT, SBERT)에 대해서 다룹니다

## 1. BERT의 문장 임베딩

BERT로부터 문장 벡터를 얻는 방법은 여러가지 방법이 존재하지만 여기서는 총 세 가지에 대해서 언급하겠습니다 만약 사전 학습된 BERT에 'I love you'라는 문장이 입력된다고 하였을 때, 이 문장에 대한 벡터를 얻는 첫번째 방법은 [CLS] 토큰의 출력 벡터를 문장 벡터로 간주하는 것입니다

![image](https://user-images.githubusercontent.com/80239748/147358760-8fb0b4a1-e3fc-4c1d-8b33-5cc486ed211c.png)

앞서 BERT로 텍스트 분류 문제를 풀 때, [CLS] 토큰의 출력 벡터를 출력층의 입력으로 사용했던 점을 상기해봅시다
이는 우리가 [CLS] 토큰이 입력된 문장에 대한 총체적 표현이라고 간주하고 있기 때문입니다

다시 말해 [CLS] 토큰 자체를 우리는 입력 문장의 벡터로 간주할 수 있습니다

![image](https://user-images.githubusercontent.com/80239748/147358783-58967f20-3d81-48ff-b92a-c674cdcdbd22.png)

문장 벡터를 얻는 두번째 방법은 [CLS] 토큰만을 사용하는 것이 아니라 BERT의 모든 출력 벡터들을 평균내는 것입니다 이 논리는 BERT에 대해서도 적용되는데, BERT의 각 단어에 대한 출력 벡터들에 대해서 평균을 내고 이를 문장 벡터로 볼 수 있습니다

위 그림에서는 출력 벡터들의 평균을 `pooling`이라고 표현했습니다 이를 우리는 평균 풀링(mean pooling)을 하였다고 표현하기도 합니다

그런데 풀링에는 평균 풀링만 있는 것이 아니라 합성곱 신경망을 다룰 때 설명했던 맥스 풀링(max pooling) 또한 존재합니다 세번째 방법은 BERT의 각 단어의 출력 벡터들에 대해서 평균 풀링 대신 맥스 풀링을 진행한 벡터를 얻는 것입니다

### 정리하면 사전 학습된 BERT로부터 문장 벡터를 얻는 방법은 다음과 같이 세 가지가 있습니다

* BERT의 [CLS] 토큰의 출력 벡터를 문장 벡터로 간주한다
* BERT의 모든 단어의 출력 벡터에 대해서 **평균 풀링** 을 수행한 벡터를 문장 벡터로 간주한다
* BERT의 모든 단어의 출력 벡터에 대해서 **맥스 풀링** 을 수행한 벡터를 문장 벡터로 간주한다

이때 평균 풀링을 하느냐와 맥스 풀링을 하느냐에 따라서 해당 문장 벡터가 가지는 의미는 다소 다른데, 평균 풀링을 얻은 문장 벡터의 경우에는 모든 단어의 의미를 반영하는 쪽에 가깝다면, 맥스 풀링을 얻은 문장 벡터의 경우에는 중요한 단어의 의미를 반영하는 쪽에 가깝습니다

## 2. SBERT(센텐스버트, Sentence-BERT)

SBERT는 기본적으로 BERT의 문장 임베딩의 성능을 우수하게 개선시킨 모델입니다 SBERT는 위에서 언급한 BERT의 문장 임베딩을 응용하여 BERT를 파인 튜닝합니다. SBERT가 어떤 식으로 학습되는지 정리해봅시다

### 1) 문장 쌍 분류 태스크로 파인 튜닝

SBERT를 학습하는 첫번째 방법은 문장 쌍 분류 태스크. 대표적으로는 NLI(Natural Language Inferencing) 문제를 푸는 것입니다 우리는 이미 앞서 한국어 버전의 NLI 데이터인 KorNLI 데이터를 풀어본 적이 있습니다

NLI는 두 개의 문장이 주어지면 수반(entailment) 관계인지, 모순(contradiction) 관계인지, 중립(neutral) 관계인지를 맞추는 문제입니다 다음은 NLI 데이터의 예시입니다

|문장 A|문장 B|레이블|
|----|----|----|
|A lady sits on a bench that is against a shopping mall.|A person sits on the seat.|Entailment|
|A lady sits on a bench that is against a shopping mall.|A woman is sitting against a building.|Entailment|
|A lady sits on a bench that is against a shopping mall.|Nobody is sitting on the bench.|Contradiction|
|Two women are embracing while holding to go packages.|The sisters are hugging goodbye while holding to go packages after just eating lunch.|Neutral|

SBERT는 NLI 데이터를 학습하기 위해 다음과 같은 구조를 가집니다

![image](https://user-images.githubusercontent.com/80239748/147378745-382e4f94-dbdf-4cf1-9e37-adf70cd96e9f.png)

우선 문장 A와 문장 B 각각을 BERT의 입력으로 넣고, 앞서 BERT의 문장 임베딩을 얻기위한 방식이라고 언급했던 평균 풀링 또는 맥스 풀링을 통해서 각각에 대한 문장 임베딩 벡터를 얻습니다

여기서는 이를 각각 u와 v라고 합시다

그리고 나서 u벡터와 v벡터의 차이 벡터를 구합니다 이 벡터는 수식으로 표현하면 |u-v|입니다 그리고 이 세 가지 벡터를 연결(concatenation)합니다 세미콜론(;)을 연결 기호로 한다면 연결된 벡터의 수식은 다음과 같습니다

![image](https://user-images.githubusercontent.com/80239748/147378764-5b7ac608-20bb-4851-af9d-2d9cf7140463.png)
