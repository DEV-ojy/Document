# 센텐스버트(Sentence BERT, SBERT)

BERT로부터 문장 임베딩을 얻을 수 있는 센텐스버트(Sentence BERT, SBERT)에 대해서 다룹니다

## 1. BERT의 문장 임베딩

BERT로부터 문장 벡터를 얻는 방법은 여러가지 방법이 존재하지만 여기서는 총 세 가지에 대해서 언급하겠습니다 만약 사전 학습된 BERT에 'I love you'라는 문장이 입력된다고 하였을 때, 이 문장에 대한 벡터를 얻는 첫번째 방법은 [CLS] 토큰의 출력 벡터를 문장 벡터로 간주하는 것입니다

![image](https://user-images.githubusercontent.com/80239748/147358760-8fb0b4a1-e3fc-4c1d-8b33-5cc486ed211c.png)

앞서 BERT로 텍스트 분류 문제를 풀 때, [CLS] 토큰의 출력 벡터를 출력층의 입력으로 사용했던 점을 상기해봅시다
이는 우리가 [CLS] 토큰이 입력된 문장에 대한 총체적 표현이라고 간주하고 있기 때문입니다

다시 말해 [CLS] 토큰 자체를 우리는 입력 문장의 벡터로 간주할 수 있습니다

![image](https://user-images.githubusercontent.com/80239748/147358783-58967f20-3d81-48ff-b92a-c674cdcdbd22.png)

문장 벡터를 얻는 두번째 방법은 [CLS] 토큰만을 사용하는 것이 아니라 BERT의 모든 출력 벡터들을 평균내는 것입니다 이 논리는 BERT에 대해서도 적용되는데, BERT의 각 단어에 대한 출력 벡터들에 대해서 평균을 내고 이를 문장 벡터로 볼 수 있습니다

위 그림에서는 출력 벡터들의 평균을 `pooling`이라고 표현했습니다 이를 우리는 평균 풀링(mean pooling)을 하였다고 표현하기도 합니다

그런데 풀링에는 평균 풀링만 있는 것이 아니라 합성곱 신경망을 다룰 때 설명했던 맥스 풀링(max pooling) 또한 존재합니다 세번째 방법은 BERT의 각 단어의 출력 벡터들에 대해서 평균 풀링 대신 맥스 풀링을 진행한 벡터를 얻는 것입니다

### 정리하면 사전 학습된 BERT로부터 문장 벡터를 얻는 방법은 다음과 같이 세 가지가 있습니다

* BERT의 [CLS] 토큰의 출력 벡터를 문장 벡터로 간주한다
* BERT의 모든 단어의 출력 벡터에 대해서 **평균 풀링** 을 수행한 벡터를 문장 벡터로 간주한다
* BERT의 모든 단어의 출력 벡터에 대해서 **맥스 풀링** 을 수행한 벡터를 문장 벡터로 간주한다

이때 평균 풀링을 하느냐와 맥스 풀링을 하느냐에 따라서 해당 문장 벡터가 가지는 의미는 다소 다른데, 평균 풀링을 얻은 문장 벡터의 경우에는 모든 단어의 의미를 반영하는 쪽에 가깝다면, 맥스 풀링을 얻은 문장 벡터의 경우에는 중요한 단어의 의미를 반영하는 쪽에 가깝습니다