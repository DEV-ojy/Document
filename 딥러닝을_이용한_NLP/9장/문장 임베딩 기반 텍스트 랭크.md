# 문장 임베딩 기반 텍스트 랭크(TextRank Based on Sentence Embedding)

앞서 추상적 요약(abstractive summarization)을 통한 텍스트 요약을 수행해보았습니다 이번 챕터에서는 텍스트랭크(TextRank) 알고리즘으로 사용하여 또 다른 텍스트 요약 방법인 추출적 요약을 진행해보겠습니다

## 1. 텍스트랭크(TextRank)

텍스트랭크 알고리즘에 대해서 이해하기 위해서, 텍스트랭크 알고리즘의 기반이 된 페이지랭크 알고리즘에 대해서 간단히 이해해보겠습니다
페이지랭크 알고리즘은 검색 엔진에서 웹 페이지의 순위를 정하기 위해 사용되던 알고리즘입니다

    작성 중입니다

텍스트랭크 알고리즘은 페이지랭크를 기반으로 한 텍스트 요약 알고리즘입니다 
텍스트랭크에서 그래프의 노드들은 문장들이며, 각 간선의 가중치는 문장들 간의 유사도를 의미합니다

## 2. 사전 훈련된 임베딩(Pre-trained Embedding)

이번 챕터에서는 사전 훈련된 임베딩을 사용합니다 워드 임베딩 방법으로는 여러가지가 있습니다 대표적으로 사용할 수 있는 임베딩 방법인 GloVe, FastText, Word2Vec의 사전 훈련된 임베딩 사용 방법은 다음과 같습니다 

각 임베딩을 다운로드하기 위해선느 어느정도 시간이 소용되므로 실습을 위해서는 우선 GloVe만 을 다운로드 하는 것을 권합니다 

```py
import numpy as np
import gensim
from urllib.request import urlretrieve, urlopen
import gzip
import zipfile
```
### 1. 사전 훈련된 GloVe 다운로드 (실습에서 사용)

```py
urlretrieve("http://nlp.stanford.edu/data/glove.6B.zip", filename="glove.6B.zip")
zf = zipfile.ZipFile('glove.6B.zip')
zf.extractall() 
zf.close()
```
```py
glove_dict = dict()
f = open('glove.6B.100d.txt', encoding="utf8") # 100차원의 GloVe 벡터를 사용

for line in f:
    word_vector = line.split()
    word = word_vector[0]
    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') # 100개의 값을 가지는 array로 변환
    glove_dict[word] = word_vector_arr
f.close()
```
만약 단어`cat`에 대한 임베딩 벡터를 얻고싶다면 다음과 같이 얻을 수 있습니다 
```py
glove_dict['cat']
```

### 2. 사전 훈련된 FastText 다운로드
```py
!pip install fasttext
```
```py
# 300차원의 FastText 벡터 사용
import fasttext.util
fasttext.util.download_model('en', if_exists='ignore')
ft = fasttext.load_model('cc.en.300.bin')
```
만약 단어 `cat`에 대한 임베딩 벡터를 얻고싶다고면 다음과 같이 얻을 수 있습니다 
```py
ft.get_word_vector('cat')
```
