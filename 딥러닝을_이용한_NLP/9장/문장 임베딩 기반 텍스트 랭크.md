# 문장 임베딩 기반 텍스트 랭크(TextRank Based on Sentence Embedding)

앞서 추상적 요약(abstractive summarization)을 통한 텍스트 요약을 수행해보았습니다 이번 챕터에서는 텍스트랭크(TextRank) 알고리즘으로 사용하여 또 다른 텍스트 요약 방법인 추출적 요약을 진행해보겠습니다

## 1. 텍스트랭크(TextRank)

텍스트랭크 알고리즘에 대해서 이해하기 위해서, 텍스트랭크 알고리즘의 기반이 된 페이지랭크 알고리즘에 대해서 간단히 이해해보겠습니다
페이지랭크 알고리즘은 검색 엔진에서 웹 페이지의 순위를 정하기 위해 사용되던 알고리즘입니다

    작성 중입니다

텍스트랭크 알고리즘은 페이지랭크를 기반으로 한 텍스트 요약 알고리즘입니다 
텍스트랭크에서 그래프의 노드들은 문장들이며, 각 간선의 가중치는 문장들 간의 유사도를 의미합니다

## 2. 사전 훈련된 임베딩(Pre-trained Embedding)

이번 챕터에서는 사전 훈련된 임베딩을 사용합니다 워드 임베딩 방법으로는 여러가지가 있습니다 대표적으로 사용할 수 있는 임베딩 방법인 GloVe, FastText, Word2Vec의 사전 훈련된 임베딩 사용 방법은 다음과 같습니다 

각 임베딩을 다운로드하기 위해선느 어느정도 시간이 소용되므로 실습을 위해서는 우선 GloVe만 을 다운로드 하는 것을 권합니다 

```py
import numpy as np
import gensim
from urllib.request import urlretrieve, urlopen
import gzip
import zipfile
```
### 1. 사전 훈련된 GloVe 다운로드 (실습에서 사용)

```py
urlretrieve("http://nlp.stanford.edu/data/glove.6B.zip", filename="glove.6B.zip")
zf = zipfile.ZipFile('glove.6B.zip')
zf.extractall() 
zf.close()
```
```py
glove_dict = dict()
f = open('glove.6B.100d.txt', encoding="utf8") # 100차원의 GloVe 벡터를 사용

for line in f:
    word_vector = line.split()
    word = word_vector[0]
    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') # 100개의 값을 가지는 array로 변환
    glove_dict[word] = word_vector_arr
f.close()
```
만약 단어`cat`에 대한 임베딩 벡터를 얻고싶다면 다음과 같이 얻을 수 있습니다 
```py
glove_dict['cat']
```

### 2. 사전 훈련된 FastText 다운로드
```py
!pip install fasttext
```
```py
# 300차원의 FastText 벡터 사용
import fasttext.util
fasttext.util.download_model('en', if_exists='ignore')
ft = fasttext.load_model('cc.en.300.bin')
```
만약 단어 `cat`에 대한 임베딩 벡터를 얻고싶다고면 다음과 같이 얻을 수 있습니다 
```py
ft.get_word_vector('cat')
```

### 3. 사전 훈련된 Word2Vec 다운로드

```py
# 300차원의 Word2Vec 벡터 사용
urlretrieve("https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz", \
                           filename="GoogleNews-vectors-negative300.bin.gz")
word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)
```
만약 단어 `cat`에 대한 임베딩 벡터를 얻고 싶다면 다음과 같이 얻을 수 있습니다 
```py
word2vec_model['cat']
```
## 3. 문장 임베딩(Sentence Embedding)
여러분이 어떤 다수의 문장을 가지고 있다고 해봅시다 그리고 이 문장들을 서로 비교하고 싶습니다 
만약 문장들을 각 문장을 표현하는 고정된 길이의 벡터로 변환한다면 벡터 간 비교로 문장을 비교할 수 있을 것입니다 

각 문장을 문장벡터로 변환하는 방법은 여러가지 방법이 존재하지만 여기서는 가장 간단한 방법 한가지를 소개하고자 합니다 

문장벡터를 얻는 가장 간단한 방법은 문장에 존재한느 단어 벡터들의 평균을 구하는 것 입니다 
앞서 다운로드한 사전 훈련된 임베딩을 사용합니다 

예를 들어 사전 훈련된 GloVe로부터 문장 벡터는 다음과 같이 얻을 수 있습니다

현재 glove_dict에는 100차원의 GloVe 벡터들이 저장되어져 있습니다. OOV 문제. 즉, glove_dict에 존재하지 않는 단어가 문장에 존재할 경우 해당 단어의 임베딩값으로 사용할 100차원의 영벡터도 만들어둡니다
```py
embedding_dim = 100
zero_vector = np.zeros(embedding_dim)
```
아래 함수는 문장의 각 단어를 사전 훈련된 GloVe 벡터로 변환하면서, OOV 문제가 발생할 경우에는 해당 단어를 영벡터로 변환합니다

그리고 이렇게 모인 단어 벡터들의 평균을 구하여 반환합니다
```py
# 단어 벡터의 평균으로부터 문장 벡터를 얻는다.
def calculate_sentence_vector(sentence):
  return sum([glove_dict.get(word, zero_vector) 
                  for word in sentence])/len(sentence)
```
만약  `I am a student`라는 문장 벡터의 값을 얻고 싶다면 해당 문장을 **calculate_sentence_vector** 함수의 입력으로 사용하면 됩니다 이렇게 반환된 벡터값의 크기는 100차원이 될 것입니다 여기서는 책의 지면의 한계로 값을 확인하진 않겠습니다 

```py
eng_sent = ['I', 'am', 'a', 'student']
sentence_vector = calculate_sentence_vector(eng_sent)
print(len(sentence_vector))
```
    100

현재 사용하고 있는 사전 훈련된 GloVe는 영어에 대해서 학습된 임베딩입니다 
그래서 한국어를 넣으면 당연히 모든 단어에 대해서 OOV 문제가 발생합니다 
즉, 모든 단어가 영벡터이므로 평균을 구해도 영벡터가 반환됩니다 실제로 값을 확인해봅시다 

```py
kor_sent = ['전', '좋은', '학생', '입니다']
sentence_vector = calculate_sentence_vector(kor_sent)
print(sentence_vector)
```

```
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0.]
 ```