# 로지스틱 회귀(Logistic Regression)

일상 속 풀자고하는 많은 문제 중에서도 두개의 선택지 중에서 정답을 고르는 문제가 많다 
합격인지 불합격인지, 정상메일인지 스팸메일인지 이렇게 둘 중 하나로 결정하는 문제를 **이진 분류(Binary Classification)** 이라고한다

이러한 이진 분류를 풀기 위한 대표적인 알고리즘으로 __로지스틱 회귀__ 가 있습니다 

## 이진 분류(Binary Classification)

학생들이 시험 성적에 따라서 합격,불합격이 기재된 데이터가 있다고 가정해봅시다 시험 성적이 x라면 합격,불합격 결과는 y입니다
이 시험의 커트라인은 공개되지 않았는데 이 데이터로부터 특정 점수를 얻었을때의 합격,불합격 여부를 판정하는 모델을 만들어보겠습니다

![image](https://user-images.githubusercontent.com/80239748/128172292-fbe33045-eaa4-49ba-ae90-181f23aba665.png)

위의 데이터에서 합격을 1,불합격을 0이라고 했을때 그래프를 그려보면 

![image](https://user-images.githubusercontent.com/80239748/128172418-d89d0c31-bfbc-4762-b0d0-9aa4a7be149b.png)

이러한 점들을 표현하는 그래프는 알파벳의 S자 헝태로 표현됩니다 이러한 x와 y의 관계를 표현하기 위해서는 Wx + b와 같은 직선함수가 아니라
S자 형태로 표현할 수 있는 함수가 필요합니다 이런 문제에 직선을 사용할 경우 분류작업이 잘 동작하지 않습니다

그래서 이번 로지스틱 회귀의 가설은 선형 회귀 때의 H(x) = Wx + b가 아니라, 위와 같은 S자 모양의 그래프를 만들 수 있는 어떤
특정 함수 f를 추가적으로 사용하여 H(x) = f(Wx + b)의 가설을 사용할 겁니다 그리고 위와 같은 S자 모양의 그래프를 그릴 수 있는
어떤 함수 f감 이미 널리 알려져있습니다 그것은 바로 시그모이드 함수입니다 

## 시그모이드 함수 (Sigmoid function)

위의 같이 S자 형태로 그래프를 그려주는 시그모이드 함수의 방정식은 아래와 같습니다 

![image](https://user-images.githubusercontent.com/80239748/128173171-a267e074-785a-4c57-ac01-276c06e73600.png)

선형 회귀에서는 최적의 W와 b를 찾는 것이 목표였습니다 여기서도 마찬가지입니다 선형 회귀에서는 W가 직선의 기울기,b가 y절편을
의미했습니다 그렇다면 여기에서는 W와 b가 함수의 그래프에 어떤 영향을 주는지 직접 그래프를 그려서 알아보겠습니다 

```py
import numpy as np # 넘파이 사용
import matplotlib.pyplot as plt # 맷플롯립사용

def sigmoid(x): # 시그모이드 함수 정의
    return 1/(1+np.exp(-x))
```

### 1.W가 1이고 b가 0인 그래프 

```py
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)

plt.plot(x, y, 'g')
plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가
plt.title('Sigmoid Function')
plt.show()
```

![image](https://user-images.githubusercontent.com/80239748/128173751-a3077f5e-7201-4894-b756-bdbc642cea99.png)

위의 그래프를 통해시그모이드 함수는 출력값을 0과1사이의 값으로 조정하여 반환함을 알 수 있습니다 x가 0일때 0.5의 값을가집니다
x가 매우 커지면 1에 수렴합니다 반면,x가 매우 작아지면 0에 수렵합니다 

### W값의 변화에 따른 경사도의 변화 

```py
x = np.arange(-5.0, 5.0, 0.1)
y1 = sigmoid(0.5*x)
y2 = sigmoid(x)
y3 = sigmoid(2*x)

plt.plot(x, y1, 'r', linestyle='--') # W의 값이 0.5일때
plt.plot(x, y2, 'g') # W의 값이 1일때
plt.plot(x, y3, 'b', linestyle='--') # W의 값이 2일때
plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가
plt.title('Sigmoid Function')
plt.show()
```

![image](https://user-images.githubusercontent.com/80239748/128174000-0057d191-7c27-4a24-94b9-19049b9c183e.png)

위의 그래프 W의 값이 0.5일때 빨간색선, W의 값이 1일때는 초록색선, W의 값이 2일때 파란색선이 나오도록 하였습니다 
자세히 보면 W의 값에 따라 그래프는 경사도가 변하는 것을 볼 수 있습니다 앞서 선형 회구에서 가중치 W는 직선의 기울기를
의미했지만, 여기서는 그래프의 경사도를 결정합니다 W의 값이 커지면 경사가 커지고 W의 값이 작아지면 경사가 작아집니다 

### 3. b값의 변화에 따른 좌,우 이동

이제 b의 값에 따라서 그래프가 어떻게 변하는지 확인해보겠습니다 

```py
x = np.arange(-5.0, 5.0, 0.1)
y1 = sigmoid(x+0.5)
y2 = sigmoid(x+1)
y3 = sigmoid(x+1.5)

plt.plot(x, y1, 'r', linestyle='--') # x + 0.5
plt.plot(x, y2, 'g') # x + 1
plt.plot(x, y3, 'b', linestyle='--') # x + 1.5
plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가
plt.title('Sigmoid Function')
plt.show()
```

![image](https://user-images.githubusercontent.com/80239748/128343094-eb23d5a9-0fed-4620-9101-240ce87e8cfb.png)

위의 그래프는 b의 값에 따라서 그래프가 좌,우로 이동하는 것을 보여줍니다 

### 4. 시그모이드 함수를 이용한 분류
시그모이드 함수는 입력값이 한없이 커지면 1에 수렴하고, 입력값이 한없이 작아지면 0에 수렵합니다 **시그모이드 함수의 출력값은 0과1사이의 값을 가지는데** 이 특성을 이용하여 분류 작업에 사용할 수 있습니다 예를 들어 임계값을 0.5라고 정해보겠습니다 출력값이 0.5이상이면 1 (True) 0.5이하면 0(False)으로 판단하도록 하고 이를 확률이라고 생각하면 해당 레이블에 속할 확률이 50%가 넘으며 해당 레이블로 판단하고, 해당 레이블에 속할 확률이 50%보다 낮으면 아니라고 판단하는 것으로 볼 수 있습니다 

## 3.비용함수( Cost function)

이제 로지스틱 회귀의 가설이 H(x) = sigmoid(Wx+b)인 것은 알았습니다 이제 최적의 W와 b를 찾을 수 있는 비용함수 (cost function)를 정의해야합니다 그런데 혹시 앞서 선형 회귀에서 배운 비용 함수인 평균 제곱 오차(Mean Square Error,MSE)를 로지스틱 회귀의 비용 함수로 그냥 사용하면 안될까요?

다음 선형회귀에서 사용했던 평균 제곱 오차의 수식입니다 

![image](https://user-images.githubusercontent.com/80239748/128343862-ce86b75b-22ac-40d0-aba9-133128616c79.png)

위의 비용함수 수식에서 가설은 이제 H(x)=Wx + b가 아니라 H(x) = sigmoid(Wx+b)입니다 그리고 이 비용 함수를 미분하면 선형 회귀때와 달리 다음의 그림과 유사한 심한 비볼록(non-convex)형태의 그래프가 나옵니다 

![image](https://user-images.githubusercontent.com/80239748/128344048-befbcf0c-dcc8-42a2-8f61-60732be6af89.png)








