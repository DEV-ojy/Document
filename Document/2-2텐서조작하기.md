# 텐서 조작하기
>벡터,행렬,텐서의 개념에 대해서 이해하고,Numpy와 파이토치로 벡터,행렬,텐서를 다루는방법을 이해한다

   
**1.벡터,행렬 그리고 텐서**
>딥러닝을 위한 가장 기본적인 수학ㅈ거 지식인 벡터,행렬,텐서에 대해서 알아봅니다
  
**2.넘파이 훑어보기**   
>파이토치는 파이썬 패키지 Numpy와 유사합니다 따라서 넘파이에 대해 간단히 살펴보겠습니다
  
**3.파이토치 텐서 선언하기**
>넘파이로 실습, 이제 파이토치 텐서 선언 방법에 대해 알아봅니다
  
**4.행렬 곱셈**
>행렬연산에 대해서 이해합니다
  
**5.다른 오퍼레이션들**
>다른 기본적인 오퍼레이션들에 대해서 이해해봅시다







# 1.백터,행렬 그리고 텐서(Vector, Matrix and Tensor)
![2-2(1)](https://user-images.githubusercontent.com/80239748/111144073-ead9e000-85c9-11eb-8822-28449b8d7d8d.JPG)

 백터,행렬,텐서 그림으로 이해하기
딥러닝을 하게되면 다루게 되는 가장 기본적인 단위는 벡터,행렬,텐서입니다
차원이 없는 값을 스칼라,1차원으로 구성된 값을 우린 벡터라고 합니다
2차원으로 구성된값을 행렬,그리고 3차원이 되면 우리는 텐서라고부른다
사실 우리는 3차원의 세상에 살고 있으므로 4차원 이상부터는 머리로 생각하기 
어렵습니다.4차원은 3차원의 텐서를 위로 쌓아 올린 모습으로 상상해보겠습니다
5차원은 4차원을 옆으로 확장한 모습,6차원은 5차원을 뒤로 확장한 모습으로 볼수있습니다
  
# 2.PyTorch Tensor Shape Convention
사실 딥러닝을 할때 다루고 있는 행렬 똔느 텐서의 크기를 고려하는 것은 항상중요합니다 여기서는 앞으로 행렬과 텐서의 크기를 표현할때 다음과 같은 방법으로 표기합니다

**2D Tensor |t| = (batch size, dim)**

> 2D차원 행렬에서 행의 크기가 batch size, 열의 크기가 dim이라는 의미

![2-2(2)](https://user-images.githubusercontent.com/80239748/111145234-612b1200-85cb-11eb-94ea-7e745d280548.JPG)

2차원 텐서의 크기를 |t|를 (batch size x dimension)으로 표현하였을 경우입니다 조금 쉽게 말하면, 그림과 같이 행의 크기가 batch size, 열의 크기가 dim이라는 의미입니다.

**3D Tensor |t| = (batch size, width,height)**
>일반적으로 자연어 처리보다 비전 분야(이미지,영상처리)를 하시게 된다면 좀 더 복잡한 텐서를 다루게 됩니다 이미지라는 것은 가로 세로 라는것이 존재합니다 그리고 여러장의 이미지,그러니깐 batch size로 구성하게 되면 아래와 같이 3차원 텐서가 됩니다. 

![2-2(3)](https://user-images.githubusercontent.com/80239748/111145300-6f792e00-85cb-11eb-820d-6ce7df1a183f.JPG)

(위의 그림은 세로 batch size, 가로는 너비 width, 안쪽으로 높이 height가 되는 것을 보여줍니다)

**3D Tensor(Typical Natural Language Processing) - NLP 분야에서의 3차원 텐서**

|t| = (batch size, length, dim)

자연어 처리는 보통(batch size,문장길이,단어 벡터의 차원)이라는 3차원 텐서를 사용합니다

![2-2(4)](https://user-images.githubusercontent.com/80239748/111146427-c16e8380-85cc-11eb-9a35-fdb8a521eed4.JPG)

# Example
아래와 같이 4개의 문장으로 구성된 전체 훈련 데이터가 있습니다

    [[나는 사과를 좋아해], [나는 바나나를 좋아해], [나는 사과를 싫어해], [나는 바나나를 싫어해]]

컴퓨터는 아직 이 상태로 '나는 사과를 좋아해'가 단어가 1개인지 3개인지 이해하지 못 합니다 우선 컴퓨터의 입력으로 사용하기 위해서는 단어별로 나눠줘야 한다.

    [['나는', '사과를', '좋아해'], ['나는', '바나나를', '좋아해'], 
    ['나는', '사과를', '싫어해'], ['나는', '바나나를', '싫어해']]

이제 훈련 데이터의 크기는 4X3의 크기를 가지는 2D 텐서입니다.컴퓨터는 텍스트보다는 숫자를 더 잘 처리할수있습니다 이제 각 단어를 벡터로 만들겁니다 아래와 같이 단어를 3차원의 벡터로 변환했다고 하겠습니다

    '나는' = [0.1, 0.2, 0.9]
    '사과를' = [0.3, 0.5, 0.1]
    '바나나를' = [0.3, 0.5, 0.2]
    '좋아해' = [0.7, 0.6, 0.5]
    '싫어해' = [0.5, 0.6, 0.7]

위 기준을 따라서 훈련 데이터를 재구성하면 아래와 같습니다.

    [[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],
    [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]],
    [[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],
    [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]

이제 훈련 데이터는 4X3의 크기를 가지는 3D 텐서입니다 이제 batch size를 2로 해보겠습니다

    첫번째 배치 #1
    [[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],
    [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]]]

    두번째 배치 #2
    [[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],
    [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]

컴퓨터는 배치 단위로 가져가서 연산을 수행합니다 그리고 현재 각 배치의 텐서의 크기는 (2 x 3 x 3)입니다 이는 (batch size,문장길이,단어 벡터의 차원)의 크기입니다.

# 2.넘파이로 텐서 만들기(벡터와 행렬 만들기)

PyTorch로 텐서를 만들어보기 전에 우선   Numpy로 텐서를 만들어보겠스니다. 우선 numpy를 임포트합니다.

    import numpy as np

Numpy로 텐서를 만드는 방법은 간단한데[숫자,숫자,숫자]와 같은 형식으로 만들고 이를 np.array()로 감싸주면 됩니다

## 1) 1D with Numpy
Numpy로 1차원 벡터를 만들어보겠습니다 
   
    t = np.array([0., 1., 2., 3., 4., 5., 6.])
    # 파이썬으로 설명하면 List를 생성해서 np.array로 1차원  array로 변환함.  
    print(t)

    [0. 1. 2. 3. 4. 5. 6.]

이제 1차원 벡터의 차원과 크기를 출력해보겠습니다
    print('Rank of t: ', t.ndim)
    print('Shape of t: ', t.shape)

    Rank of t:  1
    Shape of t:  (7,)

.ndim은 몇 차원인지를 출력합니다 1차원은 벡터.2차원은 행렬,3차원은 3차원 텐서였습니다 현재는 벡터이므로 1차원 출력됩니다 .shape는 크기를 출력합니다.


## 1-1)Numpy 기초 이해하기

이제 Numpy에서 각 벡터의 원소에 접근하는 방법을 알아보겠습니다 Numpy에서 인덱스 0부터 시작합니다

    print('t[0] t[1] t[-1] = ', t[0], t[1], t[-1]) # 인덱스를 통한 원소 접근

    t[0] t[1] t[-1] =  0.0 1.0 6.0

위의 결과는 0번 인덱스를 가진 원소인 0.0, 1번 인덱스를 가진 원소인 6.0이 출력되는 것을 보여줍니다 -1번 인덱스는 맨 뒤에서부터 시작하는 인덱스입니다

범위 지정으로 원소를 불러올 수도 있습니다 이를 슬라이싱(Slicing)이라고 합니다 사용 방법은 [시작번호 : 끝 번호]를 통해 사용합니다 주의할점은 슬라이싱은 [시작번호 : 끝 번호]라고 했을때 끝번호에 해당하는 것은 포함하지 않습니다

    print('t[2:5] t[4:-1]  = ', t[2:5], t[4:-1]) # [시작 번호 : 끝 번호]로 범위= 지정을 통해 가져온다.
    t[2:5] t[4:-1]  =  [2. 3. 4.] [4. 5.]

위의 슬라이싱의 결과를 보겠습니다 [2:5]라고 한다면 2번인덱스부터 4번인덱스까지의 결과를 가져온다는 의미입니다 [4:-1]은 4번인덱스부터 끝에서 첫번째 것까지의 결과를 가져온다는 의미입니다

시작번호또는 끝번호를 생략해서 슬라이싱을 하기도 합니다 [시작번호:끝번호]에서 시작번호를 생략하면 처음부터 끝번호까지 뽑아내고 끝번호를 생량하면 시작번호부터 끝까지 봅아냅니다

    print('t[:2] t[3:]     = ', t[:2], t[3:]) # 시작 번호를 생략한 경우와 끝 번호를 생략한 경우
    t[:2] t[3:]     =  [0. 1.] [3. 4. 5. 6.]

## 2D with Numpy
Numpy로 2차원  행렬을 만들어보겠습니다

    t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])
    print(t)
    [[ 1.  2.  3.]
    [ 4.  5.  6.]
    [ 7.  8.  9.]
    [10. 11. 12.]]
    print('Rank  of t: ', t.ndim)
    print('Shape of t: ', t.shape)
    Rank  of t:  2
    Shape of t:  (4, 3)

.ndim은 몇 차원인지를 출력합니다. 1차원은 벡터, 2차원은 행렬, 3차원은 3차원 텐서였습니다. 현재는 행렬이므로 2차원이 출력됩니다. .shape는 크기를 출력합니다. (4, 3)입니다. 다른 표현으로는 (4 × 3)입니다. 이는 행렬이 4행 3열임을 의미합니다.

Numpy로도 3차원 텐서도 만들 수는 있지만 이 시점에서 Numpy와 PyTorch를 비교하기 위해 PyTorch로 실습을 넘어갑니다.

# 파이토치 텐서 선언하기(PyTorch Tensor Allocation)

파이토치는 Numpy와 매우 유사합니다 하지만 더 낫습니다(better). 우선 torch를 임포트 합니다

    import torch

Numpy를 사용하여 진해했던실습ㅇ르 파이토치로 똑같이 해봅시다

## 1D with PyTorch

파이토치로 1차원 벡터를 만들어봅시다

    t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])
    print(t)

dim()을 사용하면 현재 텐서의 차원을 보여줍니다 shape나 size()를 사용하면 크기를 확인할 수 있습니다

    print(t.dim())  # rank. 즉, 차원
    print(t.shape)  # shape
    print(t.size()) # shape
   
    1
    torch.Size([7])
    torch.Size([7])

현재 1차원 텐서이며, 원소는 7개입니다. 인덱스로 접근하는 것과 슬라이싱을 해봅시다 방법은 Numpy 실습과 같습니다ㅣ

    print(t[0], t[1], t[-1])  # 인덱스로 접근
    print(t[2:5], t[4:-1])    # 슬라이싱
    print(t[:2], t[3:])       # 슬라이싱
    
    tensor(0.) tensor(1.) tensor(6.)
    tensor([2., 3., 4.]) tensor([4., 5.])
    tensor([0., 1.]) tensor([3., 4., 5., 6.])

## 2D with PyTorch

파이토치를 2차원 행렬을 만들어봅시다

    t = torch.FloatTensor([[1., 2., 3.],
                       [4., 5., 6.],
                       [7., 8., 9.],
                       [10., 11., 12.]
                      ])
    print(t)
    tensor([[ 1.,  2.,  3.],
            [ 4.,  5.,  6.],
            [ 7.,  8.,  9.],
            [10., 11., 12.]])

dim()을사용하면 헌재 텐서의 차원을 보여줍니다 size()를 사용하면 크기를 확인할수 있습니다.

    print(t.dim())  # rank. 즉, 차원
    print(t.size()) # shape
    2
    torch.Size([4, 3])

현재 텐서의 차원은 2차원이며,(4,3)의 크기를 가집니다 슬라이싱을 해봅시다

    print(t[:, 1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원의 첫번째 것만 가져온다.
    print(t[:, 1].size()) # ↑ 위의 경우의 크기
    tensor([ 2.,  5.,  8., 11.])
    torch.Size([4])

위의 결과는 첫번째 차원을 전체 선택하고, 그 상황에서 두번째 차원의 1번 인덱스 값만을 가져온 경우를 보여줍니다 다시 말해 텐서에서 두번째 열에 있는 모든 값을 가져온 상황입니다 그리고 이렇게 값을 가져온 경우 크기는 4입니다(1차원벡터)

    print(t[:, :-1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원에서는 맨 마지막에서 첫번째를 제외하고 다 가져온다.
    tensor([[ 1.,  2.],
            [ 4.,  5.],
            [ 7.,  8.],
            [10., 11.]])

위의 결과는 첫번째 차원을 전체 선택한 상황에서 두번째 차원에서는 맨 마지막에서 첫번째를 제외하고 다 가져오는 경우입니다.
