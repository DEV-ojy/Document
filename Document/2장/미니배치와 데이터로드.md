# 미니 배치와 데이터 로드 (Mini Batch and Data Load)

이번 챕터에서는 데이터를 로드하는 방법과 미니 배치 경사 하강법에 대해서 알아보겠습니다 

## 미니 배치와 배치 크기

앞서 배운 다중 선형 회귀에서 사용했던 데이터를 상기해봅니다

```py
x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 90],
                             [96, 98, 100],
                             [73, 66, 70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])
```

위 데이터의 샘플의 개수는 5개입니다. 전체 데이터를 하나의 행렬로 선언하여 전체 데이터에 대해서 경사 하강법을 수행하여 학습할 수 있습니다. 
그런데 위 데이터는 현업에서 다루게 되는 방대한 양의 데이터에 비하면 굉장히 적은 양입니다. 
만약, 데이터가 수십만개 이상이라면 전체 데이터에 대해서 경사 하강법을 수행하는 것은 매우 느릴 뿐만 아니라 
많은 계산량이 필요합니다. 정말 어쩌면 메모리의 한계로 계산이 불가능한 경우도 있을 수 있습니다.

그렇기 때문에 **전체 데이터를 더 작은 단위로 나누어서 해당 단위로 학습하는 개념이 나오게 되었습니다.
이 단위를 미니 배치(Mini Batch)라고 합니다.**

![image](https://user-images.githubusercontent.com/80239748/127742437-c3635e95-b51d-4606-9cf0-cd44cfb93d89.png)

위의 그림은 전체 데이터를 미니 배치 단위로 나누는 것을 보여줍니다 미니 배치 학습을 하게 되면 미니 배치만큼만 가져가서 
미니배치에 대한 대한비용을 계산하고 경사 하강법을 수행합니다 그리고 다음 미니 비치를 가져가서 경사 하강법을 수행하고
마지막 미니 배치까지 이를 반복합니다 이렇게 전체 데이터에 대한 학습이 1회 끝남녀 1 에포크가 끝나게 됩니다 

미니 배치 학습에서는 미니 배치의 개수만큼 경사 하강법을 수행해야 전체 데이터가 한 번 전부 사용되어 1 에포크(Epoch)가 됩니다. 미니 배치의 개수는 결국 미니 배치의 크기를 몇으로 하느냐에 따라서 달라지는데 미니 배치의 크기를 배치 크기(batch size)라고 합니다.

## 이터레이션(Iteration)

미니 배치와 배치 크기의 정의에 대해서 이해하였다면 이터레이션(iteration)을 정의할 수 있습니다.

![image](https://user-images.githubusercontent.com/80239748/127762275-96631dcc-ca53-4172-9b21-c95e9ebf3e24.png)

위의 그림은 에포크와 배치 크기와 이터레이션의 관계를 보여줍니다. 위의 그림의 예제를 통해 설명해보겠습니다.

이터레이션은 한 번의 에포크 내에서 이루어지는 매개변수인 가중치 와 의 업데이트 횟수입니다. 전체 데이터가 2,000일 때 배치 크기를 200으로 한다면 이터레이션의 수는 총 10개입니다. 이는 한 번의 에포크 당 매개변수 업데이트가 10번 이루어짐을 의미합니다.

이제 미니 배치 학습을 할 수 있도록 도와주는 파이토치의 도구들을 알아봅시다.



