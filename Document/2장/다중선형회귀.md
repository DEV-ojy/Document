# 다중 선형 회귀 (Multivariable Linear regression)

앞서 배운 x가 1개인 선형회귀를 단순 선형 회귀(Simple Linear Regression)이라고 합니다
이번 챕터에서는 다수의 x로부터 y를 예측하는 다중 선형 회귀(Multivariable Linear regression)에 대해서 이해해봅시다

## 데이터에 대한 이해(Data Definition)

다음과 같은 훈련 데이터가 있습니다 앞서 배운 단순 선형 회귀와 다른 점은 독립변수 x의 개수가 이제 1개가 아닌 3개라는점입니다
3개의 퀴즈 점수로부터 최종 점수를 예측하는 모델을 만들어보겠습니다

![image](https://user-images.githubusercontent.com/80239748/127311461-8e0c44a9-8fc8-47a5-b65b-56bc0c30f183.png)

독립 변수 x의 개수가 3개미으모 이를 수식으로 표현하면 아래와 같습니다

![image](https://user-images.githubusercontent.com/80239748/127311521-46f3dad2-531a-43cf-b5fa-313c9f21a683.png)

## 파이토치로 구현하기 

우선 필요한 도구들을 임포트하고 랜덤시드를 고정

```py

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

```

이제 훈련데이터를 선언해보겠습니다 

![image](https://user-images.githubusercontent.com/80239748/127311741-29e8608e-7882-4b31-a3f3-a55431f530c4.png)

위의 식을 보면 이번에는 단순 선형 회귀와 다르게 x의 개수가 3개입니다 그러니까 x를 3개 선언합니다

```py

# 훈련 데이터
x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])
x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])
x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])

```

이제 가중치 w와 편향 b를 선언합니다 이때 가중치 w도 3개를 선언해주어야 합니다

```py

# 가중치 w와 편향 b 초기화
w1 = torch.zeros(1, requires_grad=True)
w2 = torch.zeros(1, requires_grad=True)
w3 = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)

```

이제 가설,비용 함수, 옵티마이저를 선언한 후에 경사 하강법을 1,000회 반복합니다

```py

# optimizer 설정
optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):

    # H(x) 계산
    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b

    # cost 계산
    cost = torch.mean((hypothesis - y_train) ** 2)

    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100번마다 로그 출력
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()
        ))

```

위의 경우 가설을 선언하는 부분인 hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b 에서도 
 x_train의 개수만큼 w와 곱해주도록 작성해준 것을 확인할 수 있습니다.
