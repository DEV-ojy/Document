# 선형 회귀(Linear Regression)

이번 챕터에서는 선형회귀 이론에 대해서 이해하고,PyTorch를 이용하여 선형회귀 모델을 만들어보겠습니다

### 1.데이터에 대한 이해

    학습할 데이터에 대해서 알아봅시다

### 2.가설 수립

    가설을 수립하는 방법에 대해서 알아봅시다

### 3.손실 계산하기

    학습 데이터를 이용해서 연속적으로 모델을 개선시키는데 이 때 손실을 이용합니다

### 4.경사 하강법

    학습을 위한 핵심 알고리즘인 경사 하강법에 대해서 이해합니다 

## 1.데이터에 대한 이해(Data Definition)
선형 회귀를 위해 사용할 예제는 공부한 시간과 점수에 대한 상관관계입니다

### 훈련 데이터셋과 테스트 데이터섹

![image](https://user-images.githubusercontent.com/80239748/125445020-85508176-343b-490a-b089-7156688a56f7.png)

어떤 학생이 1시간 공부를 했더니 2점, 다른 학생이 2시간 공부를 했더니 4점, 또 다른 학생이 3시간을 공부ㅁ했더니 6점을 맞았다. 그렇다면, **내가 4시간을 공부한다면 몇 점을 맞을 숫 있을까요?**

![image](https://user-images.githubusercontent.com/80239748/125445233-1bbd810c-bb81-496f-b669-f5bc14749076.png)

이 질문에 대답하기 위해서 1시간,2시간,3시간을 공부했을 때 각각 2점,4점,6점이 나왔다는 앞서 나온 정보를 이용해야 합니다. 이때 예측을 위해 사용하는 데이터를 **훈련 데이터셋(training dataset)**이라고 합니다
학습이 끝난 후, 이 모델이 얼마나 잘 작동하는지 판별하는 데이터셋을 **테스터 데이터셋(test dataset)이라고 합니다**

## 가설(Hypothesis) 수립

머신러닝에서 식을 세울때 이 식을 **가설(Hypothesis)**이라고 합니다 보통 머신러닝에서 가설은 임의로 추축해서 세워보는 식일수도 있고 경험적으로 알고 있는 식일 수 도 있습니다 그리고 맞는 가설이 아니라고 판단되면 계속 수정해나가게 되는 식이기도 합니다

선형회귀의 가설은 이미 널리 알려져있으므로 고민할 필요가 없습니다 선ㄴ형회귀란 학습데이터와 가장 잘 맞는 하나의 직선을 찾는 일입니다 이때 선형회귀의 가설(직선의 방정식)은 이러한 형식을 가집니다

<img src="https://user-images.githubusercontent.com/80239748/125446179-fa9b639b-0e19-412c-a4c5-82432e7cab6b.png" width="280" height="100"> 

가설의 **H**를 따서 y대신 쓰기도 하고 

이때 x와 곱해지는 **W를 가중치(Weight)**라고 하며, b는 편향(bias)이라고 합니다  

## 비용함수 (Cost function)에 대한 이해

앞으로 딥 러닝을 학습하면서 인터넷에서 이런 용어들을 본다면, 전부 같은 용어로 생각하시면 됩니다

```
비용함수(cost function) = 손실함수(loss function) = 오차함수(error function) = 목적함수(objective function)
```

비용함수에 대해서 이해하기 위해서 여기서만 잠깐 새로운 예제를 사용해보겠습니다 
어떤 4개의 훈련 데이터가 있고, 이를 2차원 그래프에 4개의 점으로 표혀한 상태이다 

![image](https://user-images.githubusercontent.com/80239748/125593373-b569bff7-06a0-49a7-85ce-21074cda9202.png)

지금 목표는 4개의 점을 가장 잘 표현하는 직선을 그리는 일이다 임의로 3개의 직선을 그려 보겠습니다

![image](https://user-images.githubusercontent.com/80239748/125593705-a1d7b65a-c632-4f64-a2b1-71206236f07d.png)

위의 그림은 서로 다른 W와 b의 값에 따라서 천차만별로 그려진 3개의 직선의 모습을 보여줍니다 이 3개의 직선 중에서 4개의 점을 가장 잘 반영한 직선은 어떤 직선 일까요? 대부분의 사람들은 검은색 직선이 가장 4개의 점에 가깝게 지나가는 느낌을 받을 것 입니다 

하지만 수학에선 느낌이란 표현을 사용하는 것은 아무런 의미가 없습니다
어떤 직선이 가장 적절한 직선인지를 수학적인 근거를 대서 표현할 수 있어야 합니다 그래서 **오차(error)** 라는 개념을 도입하겠습니다

![image](https://user-images.githubusercontent.com/80239748/125717924-92e4b96c-3a1f-4c04-a4a4-f3024f97c7b3.png)

위 그림은 임의로 그려진 주황색 선에 대해서 각 실제값과 직선의 예측값에 대한 값의 차이를 빨간색 화살표로 표현한 것입니다 이를 오차라고 할 수 있습니다 그러나 이 직선의 예측값들과 실제값들과의 총 오차는 어떻게 구할까요 직관적으로 생각하기에는 모든 오차를 다 더하면 될 것 같지만 

'오차 = 실제값 - 예측값' 에서 나온 오차들을 모두 더하면 덧셈과정에서 오차값이 +가 되었다가 - 됐다가 하므로 제대로 된 오차의 크기를 측정할 수 없게 됩니다 그래서 오차를 그냥 더하는 것이 아닌 각 오차들 제곱한 다음 전부 더해 주는 것입니다 

이를 수식으로 표현하면 아래와 같습니다 **단, 여기서 n은 갖고 있는 데이터의 개수를 의미하는 것 입니다**

![image](https://user-images.githubusercontent.com/80239748/125724121-7c528927-fa42-4439-bead-275020a018ff.png)

이때 데이터의 개수인 n으로 나누면, 오차의 제곱합에 대한 평균을 구할 수 있는데 이를 **평균 제곱 오차(Mean Squared Error, MSE**라고 합니다 

![image](https://user-images.githubusercontent.com/80239748/125724223-c3d618e7-6467-4233-bafc-74ab4ed9b7b4.png)

이를 실제로 계산을 하면 52.5가 됩니다 이는 y=13x+1의 예측값과 실제값의 평균제곱 오차의 값이 52.5임을 의미합니다 

평균 제곱 오차는 이번 회귀 문제에서 적절한 W와b를 찾기위해서 최적화된 식입니다 그 이유는 평균제곱 오차의 값을 최소값으로 만느는 W와 b를 찾아내는 것이 가장 훈련데이터를 잘 반영한 직선을 찾아내는 일이기 때문입니다 

평균 제곱 오차를 W와b에 의한 비용 함수로 재정의해보면 다음과 같습니다 

![image](https://user-images.githubusercontent.com/80239748/125724457-4eb66487-9a01-46e3-bb36-0534d22e6c2d.png)

다시 정리해보면 Cost(W,b)를 최소가 되게 만드는 W와 b를 구하면 훈련데이터를 가장 잘 나타내는 직선을 구할 수 있습니다

## 옵티마이저 - 경사 하강법(Gradient Descent)
이제 앞서 정의한 비용함수의 값을 최소로 하는 W와b를 찾는 방법에 대해서 배울 차례입니다 이때 사용되는 것이 옵티마이저(Optimizer)알고리즘입니다 
**최적화 알고리즘**이라고도 부르기도 합니다 그리고 옵티마이저 알고리즘을 통해 적절한 W와b를 찾아내는 과정을 머신 러닝에서 학습이라고 부릅니다 여기서는 가장 기본적인 옵티마이저 알고리즘인 경사 하강법에 해대서 배웁니다 

이번설명에서 편향b는 고려하지 않겠습니다 즉 b가 0이라고 가정한 y=Wx와 같은 식ㅇ르 기준으로 설명합니다 

![image](https://user-images.githubusercontent.com/80239748/125732530-58063842-3f51-4626-bc48-fb99e5861841.png)

가중치 W가 직선의 방정식에서는 기울기였음을 기억합시다 이제 W를 기울기라고 명명하고 설명ㅎ바니다 

위의 그림에서 주황색선은 기울기 W가 20일 때, 초록색선은 기울기 W가 1일때를 보여줍니다 다시 말하면 각각 y=20x,y=x에 해당되는 직선 입니다 
↕는 각 점에서의 실제값과 두 직선의 예측값과의 오차를 보여줍니다 이는 앞서 예측에 사용했던 y=13x+1 직선보다 확연히 큰 오차값들입니다 즉, 기울기가 지나치게 크면 실제값과 예측값의 오차가 커지고, 기울기가 지나치게 작아도 실제값과 예측값의 오차가 커집니다 사실 b 또한 마찬가지인데 b가 지나치게 크거나 작으면 오차가 커집니다 

설명의 편의를 위해 편향b가 없이 단순히 가중치 W만을 사용한 H(x)=Wx라는 가설을 가지고, 경사 하강법을 설명하겠습니다 비용함수의값 cost(W)는 cost라고 줄여서 표현해보겠습니다 이에 따라 W와 cost의 관계를 그래프로 표현하면 다음과 같습니다

![image](https://user-images.githubusercontent.com/80239748/125733141-2cde8477-0180-40be-b7be-acec3fd7ebc5.png)

기울기가 W가 무한대로 커지면 커질수록 cost의 값 또한 무한대로 커지고 반대로 기울기 W가 무한대로 작아져도 cost의 값은 무한대로 커집니다 위의 그래프에서 cost가 가장 작을 때는 맨 아래의 볼록한 부부입니다 기계가 해야할 일은 cos가 가장 최소값을 가지게 하는 W를 찾는 일이므로 맨 아래의 볼록한 부분의 W의 값을 찾아야합니다

![image](https://user-images.githubusercontent.com/80239748/125957523-f07d0dc8-cd87-40fe-83d0-428c23e84a95.png)

기계는 임의의 초기값 W값을 정한 뒤에, 맨 아래의 볼록한 부분을 향해 점차 W의 값을 수정해 나갑니다 위의 그림은 W값이 점차 수정되는 과정을 보여줍니다 그리고 이를 가능하게 하는 것이 **경사 하강법(Gradient Descent)**입니다 이를 이해하기 위해 서는 고등학교 수학 과정인 미분을 이해야합니다 경사 하강법은 미분을 배우게 되면 가장 처음 배우게 되는 개념인 한 점에서의 순간 변화율 또는 접선에서의 기울기의 개념을 사용합니다

![image](https://user-images.githubusercontent.com/80239748/125957895-a55029a1-845b-4ec4-a48e-0b1ce02ff9d7.png)

위의 그림에서 초록색 선의 W가 임의의 값을 가지게 되는 네 가지의 경우에대해서 그래프상으로 접선의 기울기를 보여줍니다 주목할 것은 맨아래의 볼록한 부분으로 갈수록 접선의 기울기가 점차 작아진다는 점입니다 그리고 맨 아래의 볼록한 부분에서는 결국 접선의 기울기가 0이됩니다 그래프 상으로는 초록색 화살표가 수평이 되는 지점입니다

즉, cost가 최소화가 되는 지점은 접선의 기울기가 0이 되는 지점이며, 또한 미분값이 0이 되는 지점입니다. 경사 하강법의 아이디어는 비용 함수(Cost function)를 미분하여 현재 W에서의 접선의 기울기를 구하고, 접선의 기울기가 낮은 방향으로 W의 값을 변경하는 작업을 반복하는 것에 있습니다.
