# 토치텍스트 튜토리얼(Torchtext tutorial) - 영어

## 1. 훈련 데이터와 테스트 데이터로 분리하기

IMDB리뷰데이터를 다운받아 훈련데이터 테스터데이터로 분리하여 csv파일로 저장해두겠습니다

```py
import urllib.request
import pandas as pd
```

데이터 다운로드 
```py
urllib.request.urlretrieve("https://raw.githubusercontent.com/
LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv", filename="IMDb_Reviews.csv")
```

다운한 IMDB리뷰 데이터를 데이터프레임에 저장하고 상위 5개의 행만 출력
```py
df = pd.read_csv('IMDb_Reviews.csv', encoding='latin1')
df.head()
```

![image](https://user-images.githubusercontent.com/80239748/131497332-134d1c59-01b8-40d7-8a86-0bb86297500b.png)

전체 샘플의 개수 
```py
print('전체 샘플의 개수 : {}'.format(len(df)))
```
```
전체 샘플의 개수 : 50000
```

전체 샘플의 개수는 50,000개입니다 절반씩 분리하여 훈련,테스트 데이터로 분리해보겠습니다 
```py
train_df = df[:25000]
test_df = df[25000:]
```

각 데이터들을 훈련데이터는 train_data.csv 파일에 테스트 데이터는 test_data.csv 파일에 저장하겠습니다
```py
train_df.to_csv("train_data.csv", index=False)
test_df.to_csv("test_data.csv", index=False)
```

## 2. 필드 정의하기(torchtext.data)

torchtext.data에는 필드라는 도구를 제공합니다 필드를 통해 앞으로 어떤 전처리를 할 것인지 정의합니다 
```py
from torchtext import data # torchtext.data 임포트

# 필드 정의
TEXT = data.Field(sequential=True,
                  use_vocab=True,
                  tokenize=str.split,
                  lower=True,
                  batch_first=True,
                  fix_length=20)

LABEL = data.Field(sequential=False,
                   use_vocab=False,
                   batch_first=False,
                   is_target=True)
```

위 코드에서 두개의 필드 객체를 정의 하나는 실제 텍스트를 위한 TEXT객체, 하나는 레이블 데이터를 위한 LABEL 객체입니다
각 인자가 의미하는 바는 다음과 같습니다 
* sequential : 시퀀스 데이터 여부. (True가 기본값)
* use_vocab : 단어 집합을 만들 것인지 여부. (True가 기본값)
* tokenize : 어떤 토큰화 함수를 사용할 것인지 지정. (string.split이 기본값)
* lower : 영어 데이터를 전부 소문자화한다. (False가 기본값)
* batch_first : 미니 배치 차원을 맨 앞으로 하여 데이터를 불러올 것인지 여부. (False가 기본값)
* is_target : 레이블 데이터 여부. (False가 기본값)
* fix_length : 최대 허용 길이. 이 길이에 맞춰서 패딩 작업(Padding)이 진행된다.


## 3. 데이터셋 만들기
```py
from torchtext.data import TabularDataset
```

필드를 지정했다면 이제 데이터셋을 만들어야 합니다 TabularDataset은 데이터를 불러오면 필드에서 정의했던 토큰화 방법으로
토큰화를 수행합니다 이때 기본적인 전처리도 함께 이루어집니다 

```py
train_data, test_data = TabularDataset.splits(
        path='.', train='train_data.csv', test='test_data.csv', format='csv',
        fields=[('text', TEXT), ('label', LABEL)], skip_header=True)
```
* path : 파일이 위치한 경로.
* format : 데이터의 포맷.
* fields : 위에서 정의한 필드를 지정. 첫번째 원소는 데이터 셋 내에서 해당 필드를 호칭할 이름, 두번째 원소는 지정할 필드.
* skip_header : 데이터의 첫번째 줄은 무시.

훈련 데이터와 테스트 데이터를 csv파일로 불러와 분리 저장했는데 데이터의 크기를 다시 확인해보겠습니다
```py
print('훈련 샘플의 개수 : {}'.format(len(train_data)))
print('테스트 샘플의 개수 : {}'.format(len(test_data)))
```

```
훈련 샘플의 개수 : 25000
테스트 샘플의 개수 : 25000
```

앞서 확인했던것처럼 각각 25,000개의 샘플이 존재합니다 vars()를 통해서 주어진 인덱스의 샘플을 확인할수있습니다
훈련 데이터의 첫번째 샘플을 확인해봅시다 
```py
print(vars(train_data[0]))
```
```
{'text': ['my', 'family', 'and', 'i', 'normally', 'do', 'not', 'watch', 'local', 'movies', 'for', 'the', 'simple', 'reason', 
         ... 중략 ...
         'movie?', 'congratulations', 'to', 'star', 'cinema!!', 'way', 'to', 'go,', 'jericho', 'and', 'claudine!!'],
'label': '1'}
```

앞서 TabularDataset의 fields 인자로 TEXT 필드는 text로 호칭하고, LABEL 필드는 label로 호칭한다고 했습니다 
실제로 위의 코드 결과는 text필드와 label 필드 두 가지로 구성됩니다 text필드에 저장된 영화리뷰를 보면 현재 토큰화가
진행된 것을 확인할수있습니다 아래의 코드 현재 데이터셋의 필드 구성을 별도 확인할수있습니다 
```py
# 필드 구성 확인.
print(train_data.fields.items())
```
```
dict_items([('text', <torchtext.data.field.Field object at 0x7f49d8f5d5c0>), 
('label', <torchtext.data.field.Field object at 0x7f49d938c6a0>)])
```
