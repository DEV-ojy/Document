# 토치텍스트 튜토리얼(Torchtext tutorial) - 영어

## 1. 훈련 데이터와 테스트 데이터로 분리하기

IMDB리뷰데이터를 다운받아 훈련데이터 테스터데이터로 분리하여 csv파일로 저장해두겠습니다

```py
import urllib.request
import pandas as pd
```

데이터 다운로드 
```py
urllib.request.urlretrieve("https://raw.githubusercontent.com/
LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv", filename="IMDb_Reviews.csv")
```

다운한 IMDB리뷰 데이터를 데이터프레임에 저장하고 상위 5개의 행만 출력
```py
df = pd.read_csv('IMDb_Reviews.csv', encoding='latin1')
df.head()
```

![image](https://user-images.githubusercontent.com/80239748/131497332-134d1c59-01b8-40d7-8a86-0bb86297500b.png)

전체 샘플의 개수 
```py
print('전체 샘플의 개수 : {}'.format(len(df)))
```
```
전체 샘플의 개수 : 50000
```

전체 샘플의 개수는 50,000개입니다 절반씩 분리하여 훈련,테스트 데이터로 분리해보겠습니다 
```py
train_df = df[:25000]
test_df = df[25000:]
```

각 데이터들을 훈련데이터는 train_data.csv 파일에 테스트 데이터는 test_data.csv 파일에 저장하겠습니다
```py
train_df.to_csv("train_data.csv", index=False)
test_df.to_csv("test_data.csv", index=False)
```

## 2. 필드 정의하기(torchtext.data)

torchtext.data에는 필드라는 도구를 제공합니다 필드를 통해 앞으로 어떤 전처리를 할 것인지 정의합니다 
```py
from torchtext import data # torchtext.data 임포트

# 필드 정의
TEXT = data.Field(sequential=True,
                  use_vocab=True,
                  tokenize=str.split,
                  lower=True,
                  batch_first=True,
                  fix_length=20)

LABEL = data.Field(sequential=False,
                   use_vocab=False,
                   batch_first=False,
                   is_target=True)
```

위 코드에서 두개의 필드 객체를 정의 하나는 실제 텍스트를 위한 TEXT객체, 하나는 레이블 데이터를 위한 LABEL 객체입니다
각 인자가 의미하는 바는 다음과 같습니다 
* sequential : 시퀀스 데이터 여부. (True가 기본값)
* use_vocab : 단어 집합을 만들 것인지 여부. (True가 기본값)
* tokenize : 어떤 토큰화 함수를 사용할 것인지 지정. (string.split이 기본값)
* lower : 영어 데이터를 전부 소문자화한다. (False가 기본값)
* batch_first : 미니 배치 차원을 맨 앞으로 하여 데이터를 불러올 것인지 여부. (False가 기본값)
* is_target : 레이블 데이터 여부. (False가 기본값)
* fix_length : 최대 허용 길이. 이 길이에 맞춰서 패딩 작업(Padding)이 진행된다.


## 3. 데이터셋 만들기
```py
from torchtext.data import TabularDataset
```

필드를 지정했다면 이제 데이터셋을 만들어야 합니다 TabularDataset은 데이터를 불러오면 필드에서 정의했던 토큰화 방법으로
토큰화를 수행합니다 이때 기본적인 전처리도 함께 이루어집니다 

```py
train_data, test_data = TabularDataset.splits(
        path='.', train='train_data.csv', test='test_data.csv', format='csv',
        fields=[('text', TEXT), ('label', LABEL)], skip_header=True)
```
* path : 파일이 위치한 경로.
* format : 데이터의 포맷.
* fields : 위에서 정의한 필드를 지정. 첫번째 원소는 데이터 셋 내에서 해당 필드를 호칭할 이름, 두번째 원소는 지정할 필드.
* skip_header : 데이터의 첫번째 줄은 무시.

훈련 데이터와 테스트 데이터를 csv파일로 불러와 분리 저장했는데 데이터의 크기를 다시 확인해보겠습니다
```py
print('훈련 샘플의 개수 : {}'.format(len(train_data)))
print('테스트 샘플의 개수 : {}'.format(len(test_data)))
```

```
훈련 샘플의 개수 : 25000
테스트 샘플의 개수 : 25000
```

앞서 확인했던것처럼 각각 25,000개의 샘플이 존재합니다 vars()를 통해서 주어진 인덱스의 샘플을 확인할수있습니다
훈련 데이터의 첫번째 샘플을 확인해봅시다 
```py
print(vars(train_data[0]))
```
```
{'text': ['my', 'family', 'and', 'i', 'normally', 'do', 'not', 'watch', 'local', 'movies', 'for', 'the', 'simple', 'reason', 
         ... 중략 ...
         'movie?', 'congratulations', 'to', 'star', 'cinema!!', 'way', 'to', 'go,', 'jericho', 'and', 'claudine!!'],
'label': '1'}
```

앞서 TabularDataset의 fields 인자로 TEXT 필드는 text로 호칭하고, LABEL 필드는 label로 호칭한다고 했습니다 
실제로 위의 코드 결과는 text필드와 label 필드 두 가지로 구성됩니다 text필드에 저장된 영화리뷰를 보면 현재 토큰화가
진행된 것을 확인할수있습니다 아래의 코드 현재 데이터셋의 필드 구성을 별도 확인할수있습니다 
```py
# 필드 구성 확인.
print(train_data.fields.items())
```
```
dict_items([('text', <torchtext.data.field.Field object at 0x7f49d8f5d5c0>), 
('label', <torchtext.data.field.Field object at 0x7f49d938c6a0>)])
```

## 단어 집합(Vocabulary) 만들기

토큰화 전처리를 끝냈다면 이제 각 단어에 고유한 정수를 맵핑해주는 정수 인코딩작업이 필요합니다 그리고 전처리를 위해서는 
우선 단어 집합을 만들어주어야 합니다 

정의한 필드에 .build_vocab() 도구를 사용하면 단어 집합을 생성합니다 

```py
TEXT.build_vocab(train_data, min_freq=10, max_size=10000)
```

* min_freq: 단어 집합에 추가 시 단어의 최소 등장 빈도 조건을 추가
* max_size: 단어 집합의 최대 크기를 지정

생성된 단어 집합의 크기를 확인해봅시다 

```py
print('단어 집합의 크기 : {}'.format(len(TEXT.vocab)))
```
```
단어 집합의 크기 : 10002
```

생성된 단어 집합 내의 단어들은 .stoi를 통해서 확인 가능합니다 
```py
print(TEXT.vocab.stoi)
```
```
defaultdict(<function _default_unk_index at 0x7fb218dc0488>,
{'<unk>': 0, '<pad>': 1, 'the': 2, 'a': 3, 'and': 4, 'of': 5, 'to': 6, 'is': 7, 'in': 8, 'i': 9, 'this': 10, 'it': 11, 'that': 
... 중략 ...
'hence,': 9997, 'here;': 9998, 'hippies': 9999, 'idea.<br': 10000, 'imitate': 10001})
```

## 5.토치텍스트의 데이터로더 만들기

데이터로더는 데이터셋에서 미니 배치만큼 데이터를 로드하게 만들어주는 역할을 합니다 
토치텍스트에서 lterator를 사용하여 데이터로더를 만듭니다
```py
from torchtext.data import Iterator
```

임의로 배치 크기를 아주 작은 배치 크기인 5로 정해보겠습니다
```py
batch_size = 5 //배치 크기 

train_loader = Iterator(dataset=train_data, batch_size = batch_size)
test_loader = Iterator(dataset=test_data, batch_size = batch_size)

print('훈련 데이터의 미니 배치 수 : {}'.format(len(train_loader)))
print('테스트 데이터의 미니 배치 수 : {}'.format(len(test_loader)))
```
```
훈련 데이터의 미니 배치 수 : 5000
테스트 데이터의 미니 배치 수 : 5000
```

훈련 데이터와 테스트 데이터 모두 미니 배치의 수가 5,000개 인데 이는 25,000개의 샘플을 배치 크기 5씩 
묶어주었기 때문입니다 첫번째 미니 배치를 받아와서 batch라는 변수에 저장해보겠습니다 

```py
batch = next(iter(train_loader)) # 첫번째 미니배치

print(type(batch))
```
```
<class 'torchtext.data.batch.Batch'>
```

앞서배운 일밙거인 데이터로더와 토치텍스트의 데이터로더는 조금 다른데 일반적인 데이터로더는 미니 배치를 텐서로 가져오지만
토치텍스트의 데이터로더는 torchtext.data.batch.Batch 객체를 가져오는 것을 볼수있습니다 실제 데이터 텐서에 접근하기
위해서는 정의한 필드명을 사용해야 합니다 

첫번째 미니 배치의 text필드를 호출해보겠습니다
```py
print(batch.text)
```
```
tensor([[ 248,   39,    0,    0,   55, 7701,    0,  174,  701,   34,    3,  403,
            8,    0, 1480,    0, 2595, 1499,    0,    9],
        [  50,    9,   82, 2294,    2,    0,   26,    6, 1130,   44,   10,  265,
           54,   28,  450,   16,   55, 5506,   18,   94],
        [  10,    7, 5302,    2,  119,   25,  206,    0,    0,   95,  309, 1578,
            3, 8885,  269, 1373,    0,    8,    0,    4],
        [   2,  256,  122,    5,   10,    0,    0, 3588,    0,    0, 4750,   19,
          386,    0,    0,  996,  135,   68,    0,    2],
        [  10,   14,   61,    3,    0,    5,    3, 3536,    9,  202,   11,   42,
         3589,  182, 1193,   19, 2608,  269,    4,  641]])
```

배치크기가 5였기때문에 5개의 샘플이 출력되는 것을 볼 수 있습니다 각 샘플의 길이는 20의 길이를 가지는데 이는
앞서 초기에 필드를 정의할때 fix_length를 20으로 정해주었기 때문에 다시 말해 하나의 미니 배치의 크기는 (배치 크기 × fix_length)입니다

또한 각 샘플은 더 이상 단어 시퀀스가 아니라 정수 시퀀스임을 볼 수 있습니다 각 단어는 단어 집합에서 정해진대로 각 단어에
맵핑되는 고유한 정수로 변환된 상태입니다 각 샘플의 중간,중간에는 숫자 0이 존재하는데 이는 <unk> 토큰의 번호로 단어
집합에 포함되지 못한 단어들은 <unk>라는 토큰으로 변환됐습니다
  
## 6. <pad>토큰이 사용되는 경우

<pad> 토큰이 어떤 경우에 사용되는지 보겠습니다 맨 처음 필드를 정의할 때 fix_length를 20이 아니라 150으로 정의하고
이후에는 데이터로더를 정의하기까지 모든 실습을 동일하게 진행했다고 가정해봅시다 
그리고 첫번째 미니배치의 첫번째샘플을 출력해보겠습니다 
  
```py
batch = next(iter(train_loader)) # 첫번째 미니배치
print(batch.text[0]) # 첫번째 미니배치 중 첫번째 샘플
```
  
```
tensor([ 248,   39,    0,    0,   55, 7701,    0,  174,  701,   34,    3,  403,
         8,    0, 1480,    0, 2595, 1499,    0,    9,  388, 5068,    6,   73,
         ... 중략 ...
         1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
         1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1])
```  
 
샘플의 뒷 부분이 <pad>의 번호였던 숫자 1로 채워진 것을 볼 수 있습니다 이는 기존 샘플의 길이가 150이 되지 않기때문에
숫자 1을 채워서 샘플의 길이를 150으로 맞춘것입니다 이와 같이 **서로 다른 길이의 샘플들을 동일한 길이** 로 맞춰주는
작업을 **패딩작업이라고한다**
